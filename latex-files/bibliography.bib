
@misc{PlanCalcul,
	title = {Plan {Calcul}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Plan_Calcul&oldid=218696510},
	abstract = {Le plan Calcul était un plan gouvernemental français lancé en 1966 par le président Charles de Gaulle sur l'impulsion de Michel Debré et d'un groupe de hauts fonctionnaires et d'industriels, destiné à assurer l'autonomie du pays dans les techniques de l'information, et à développer une informatique française, puis européenne dans le cadre d’Unidata.},
	language = {fr},
	urldate = {2024-11-25},
	journal = {Wikipédia},
	month = sep,
	year = {2024},
	note = {Page Version ID: 218696510},
	file = {Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\XN3LVFBB\\Plan_Calcul.html:text/html},
}

@misc{inria_history,
	title = {Notre histoire {\textbar} {Inria}},
	url = {https://www.inria.fr/fr/notre-histoire},
	urldate = {2024-11-25},
	file = {Notre histoire | Inria:C\:\\Users\\Nathan\\Zotero\\storage\\KBY5GBBG\\notre-histoire.html:text/html},
}

@misc{inria_microsoft,
	title = {Inria {Joint} {Center}},
	url = {https://www.microsoft.com/en-us/research/collaboration/inria-joint-centre/},
	abstract = {The Centre's objective is to pursue fundamental, long-term research in formal methods, software security, and the application of computer science research to the sciences.},
	language = {en-US},
	urldate = {2024-11-25},
	journal = {Microsoft Research},
	file = {Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\T2MBL657\\inria-joint-centre.html:text/html},
}

@misc{numpex,
	title = {Numérique pour l’{Exascale} ({NumPEx})},
	url = {https://anr.fr/fr/france-2030/programmes-et-equipements-prioritaires-de-recherche-pepr/numerique-pour-lexascale-numpex/},
	language = {fr},
	urldate = {2024-11-25},
	journal = {Agence nationale de la recherche},
}


@misc{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	url = {http://arxiv.org/abs/1802.03268},
	doi = {10.48550/arXiv.1802.03268},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	month = feb,
	year = {2018},
	note = {arXiv:1802.03268},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\CUHM29GB\\Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter Sharing.pdf:application/pdf;Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\36JGKKTC\\1802.html:text/html},
}


@misc{wang_bayesian_2014,
	title = {Bayesian {Multi}-{Scale} {Optimistic} {Optimization}},
	url = {http://arxiv.org/abs/1402.7005},
	abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and Freitas, Nando de},
	month = feb,
	year = {2014},
	note = {arXiv:1402.7005 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/HYMD2UAY/Wang et al. - 2014 - Bayesian Multi-Scale Optimistic Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/TZ6K6S44/1402.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/9LX3T746/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/A46MMFEF/1412.html:text/html},
}

@article{talbi_automated_2021,
	title = {Automated {Design} of {Deep} {Neural} {Networks}: {A} {Survey} and {Unified} {Taxonomy}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Automated {Design} of {Deep} {Neural} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3439730},
	doi = {10.1145/3439730},
	abstract = {In recent years, research in applying optimization approaches in the automatic design of deep neural networks has become increasingly popular. Although various approaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this hot research topic. In this article, we propose a unified way to describe the various optimization algorithms that focus on common and important search components of optimization algorithms: representation, objective function, constraints, initial solution(s), and variation operators. In addition to large-scale search space, the problem is characterized by its variable mixed design space, it is very expensive, and it has multiple blackbox objective functions. Hence, this unified methodology has been extended to advanced optimization approaches, such as surrogate-based, multi-objective, and parallel optimization.},
	number = {2},
	urldate = {2024-11-20},
	journal = {ACM Comput. Surv.},
	author = {Talbi, El-Ghazali},
	month = mar,
	year = {2021},
	pages = {34:1--34:37},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/4HF649T2/Talbi - 2021 - Automated Design of Deep Neural Networks A Survey and Unified Taxonomy.pdf:application/pdf},
}

@unpublished{firmin_fractal-based_2022,
	title = {A fractal-based decomposition framework for continuous optimization},
	url = {https://hal.science/hal-04474444},
	abstract = {In this paper, we propose a generic algorithmic framework which defines a unified view of fractal decomposition algorithms for continuous optimization. Fractals allow building a hierarchical decomposition of the decision space by using a self-similar geometrical object. The proposed generic framework is made of five distinct and independent search components: fractal geometrical object, tree search, scoring, exploration and exploitation. The genericity of the framework allowed the instantiation of popular algorithms from the optimization, machine learning and computational intelligence communities. Moreover, new optimization algorithms can be designed using various strategies of the search components. This shows the modularity of the proposed algorithmic framework. The computational experiments emphasize the behaviors of fractal-based approaches in terms of scalability, robustness, and the balance between exploitation and exploration in the search space. The obtained results show the significance of each search component of the fractal framework, and the necessity to build harder and well-defined benchmarks which can assess the performance of deterministic, axis-aligned and symmetrical decomposition-based algorithms.},
	urldate = {2024-11-20},
	author = {Firmin, Thomas and Talbi, El-Ghazali},
	month = jul,
	year = {2022},
	keywords = {Continuous optimization, Decomposition, Fractal, High-dimensional optimization, Metaheuristic, Tree search},
	file = {HAL PDF Full Text:/home/jan/snap/zotero-snap/common/Zotero/storage/LXP2FJ8Z/Firmin and Talbi - 2022 - A fractal-based decomposition framework for continuous optimization.pdf:application/pdf},
}

@misc{tribes_hyperparameter_2024,
	title = {Hyperparameter {Optimization} for {Large} {Language} {Model} {Instruction}-{Tuning}},
	url = {http://arxiv.org/abs/2312.00949},
	doi = {10.48550/arXiv.2312.00949},
	abstract = {The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the {\textbackslash}nomad algorithm, achieving a boost in performance and human alignment of the tuned model.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Tribes, Christophe and Benarroch-Lelong, Sacha and Lu, Peng and Kobyzev, Ivan},
	month = jan,
	year = {2024},
	note = {arXiv:2312.00949},
	keywords = {Computer Science - Computation and Language, Fine-tuning, HPO, LLM, Mathematics - Optimization and Control},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/LCC9QEWN/Tribes et al. - 2024 - Hyperparameter Optimization for Large Language Model Instruction-Tuning.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/9R9QNLD4/2312.html:text/html},
}

@article{javaheripi_litetransformersearch_2022,
	title = {{LiteTransformerSearch}: {Training}-free {Neural} {Architecture} {Search} for {Efficient} {Language} {Models}},
	volume = {35},
	shorttitle = {{LiteTransformerSearch}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9949e6906be6448230cdba9a4cb2d564-Abstract-Conference.html},
	language = {en},
	urldate = {2024-11-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Javaheripi, Mojan and de Rosa, Gustavo and Mukherjee, Subhabrata and Shah, Shital and Religa, Tomasz and Teodoro Mendes, Caio Cesar and Bubeck, Sebastien and Koushanfar, Farinaz and Dey, Debadeepta},
	month = dec,
	year = {2022},
	keywords = {LLM, NAS},
	pages = {24254--24267},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/XEI96IIU/Javaheripi et al. - 2022 - LiteTransformerSearch Training-free Neural Architecture Search for Efficient Language Models.pdf:application/pdf},
}

@article{klein_structural_2023,
	title = {Structural {Pruning} of {Large} {Language} {Models} via {Neural} {Architecture} {Search}},
	url = {https://openreview.net/forum?id=VAwgL8kPvr},
	abstract = {Large language models (LLMs) mark the state-of-the-art for natural language understanding. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores weight-sharing based neural architecture search (NAS) as a form of structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process. Our NAS approach achieves up to 50\% compression with less than 5\% performance drop for a fine-tuned BERT model on 7 out of 8 text classification tasks.},
	language = {en},
	urldate = {2024-11-20},
	author = {Klein, Aaron and Golebiowski, Jacek and Ma, Xingchen and Perrone, Valerio and Archambeau, Cedric},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/JXYIXK39/Klein et al. - 2023 - Structural Pruning of Large Language Models via Neural Architecture Search.pdf:application/pdf},
}

@book{foster_deep_2024,
	address = {Paris},
	edition = {Illustrated édition},
	title = {Deep learning génératif},
	isbn = {978-2-412-09269-9},
	abstract = {L'intelligence artificielle générative est le sujet en pointe dans le monde des hautes technologies, notamment grâce à ChatGPT. Très bon démarrage de ChatGPT pour les Nuls , avec plus de 2 000 ex. vendus en 3 mois.Ce guide pratique s'adresse aux ingénieurs de l'apprentissage machine ( machine learning) et aux data scientists qui veulent créer des modèles d'apprentissage profond ( deep learning) génératifs en partant de zéro. Après une présentation des bases du deep learning, à vous les architectures de réseaux de neurones les plus sophistiquées ! Auto-encodeurs variationnels, réseaux antagonistes génératifs et systèmes d'entraînement du langage n'auront bien plus de secrets pour vous. Ce livre explore toutes les potentialités du l'IA générative, en matière de texte, de musique ou d'image, ainsi que les perspectives qu'elle ouvre aux entreprises.},
	language = {Français},
	publisher = {First Interactive},
	author = {Foster, David and Maniez, Dominique},
	year = {2024},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-11-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/BIG93IT6/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{zhang_instruction_2024,
	title = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10792},
	doi = {10.48550/arXiv.2308.10792},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT){\textbackslash}footnote\{In this paper, unless specified otherwise, instruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).\}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	month = nov,
	year = {2024},
	note = {arXiv:2308.10792},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/GLBQRRNH/Zhang et al. - 2024 - Instruction Tuning for Large Language Models A Survey.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/HQG7CH56/2308.html:text/html},
}

@misc{noauthor_llama_nodate,
	title = {The {Llama} 3 {Herd} of {Models} {\textbar} {Research} - {AI} at {Meta}},
	url = {https://ai.meta.com/research/publications/the-llama-3-herd-of-models/},
	urldate = {2024-11-20},
}

@misc{wu_evolutionary_2024,
	title = {Evolutionary {Computation} in the {Era} of {Large} {Language} {Model}: {Survey} and {Roadmap}},
	shorttitle = {Evolutionary {Computation} in the {Era} of {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2401.10034},
	doi = {10.48550/arXiv.2401.10034},
	abstract = {Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wu, Xingyu and Wu, Sheng-hao and Wu, Jibin and Feng, Liang and Tan, Kay Chen},
	month = may,
	year = {2024},
	note = {arXiv:2401.10034},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, EA, LLM, Survey},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/K4C3W7Q3/Wu et al. - 2024 - Evolutionary Computation in the Era of Large Language Model Survey and Roadmap.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/38C9TJK8/2401.html:text/html},
}

@article{gao_autobert-zero_2022,
	title = {{AutoBERT}-{Zero}: {Evolving} {BERT} {Backbone} from {Scratch}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{AutoBERT}-{Zero}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21311},
	doi = {10.1609/aaai.v36i10.21311},
	abstract = {Transformer-based pre-trained language models like BERT and its variants have recently achieved promising performance in various natural language processing (NLP) tasks. However, the conventional paradigm constructs the backbone by purely stacking the manually designed global self-attention layers, introducing inductive bias and thus leads to sub-optimal. In this work, we make the first attempt to automatically discover novel pre-trained language model (PLM) backbone on a flexible search space containing the most fundamental operations from scratch. Specifically, we propose a well-designed search space which (i) contains primitive math operations in the intra-layer level to explore novel attention structures, and (ii) leverages convolution blocks to be the supplementary for attentions in the inter-layer level to better learn local dependency. To enhance the efficiency for finding promising architectures, we propose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm, which optimizes both the search algorithm and evaluation of candidate models. Specifically, we propose Operation-Priority (OP) evolution strategy to facilitate model search via balancing exploration and exploitation. Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for fast model evaluation. Extensive experiments show that the searched architecture (named AutoBERT-Zero) significantly outperforms BERT and its variants of different model capacities in various downstream tasks, proving the architecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set.},
	language = {en},
	number = {10},
	urldate = {2024-11-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gao, Jiahui and Xu, Hang and Shi, Han and Ren, Xiaozhe and Yu, Philip L. H. and Liang, Xiaodan and Jiang, Xin and Li, Zhenguo},
	month = jun,
	year = {2022},
	note = {Number: 10},
	keywords = {LLM, NAS, Speech \& Natural Language Processing (SNLP)},
	pages = {10663--10671},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/57VWRI7L/Gao et al. - 2022 - AutoBERT-Zero Evolving BERT Backbone from Scratch.pdf:application/pdf},
}

@article{liu_survey_2023,
	title = {A {Survey} on {Evolutionary} {Neural} {Architecture} {Search}},
	volume = {34},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/2008.10937},
	doi = {10.1109/TNNLS.2021.3100554},
	abstract = {Deep Neural Networks (DNNs) have achieved great success in many applications. The architectures of DNNs play a crucial role in their performance, which is usually manually designed with rich expertise. However, such a design process is labour intensive because of the trial-and-error process, and also not easy to realize due to the rare expertise in practice. Neural Architecture Search (NAS) is a type of technology that can design the architectures automatically. Among different methods to realize NAS, Evolutionary Computation (EC) methods have recently gained much attention and success. Unfortunately, there has not yet been a comprehensive summary of the EC-based NAS algorithms. This paper reviews over 200 papers of most recent EC-based NAS methods in light of the core components, to systematically discuss their design principles as well as justiﬁcations on the design. Furthermore, current challenges and issues are also discussed to identify future research in this emerging ﬁeld.},
	language = {en},
	number = {2},
	urldate = {2024-11-20},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Tan, Kay Chen},
	month = feb,
	year = {2023},
	note = {arXiv:2008.10937 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, EA, NAS},
	pages = {550--570},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/K7EK788T/Liu et al. - 2023 - A Survey on Evolutionary Neural Architecture Search.pdf:application/pdf},
}

@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/677M3UYR/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/GP46FG6G/2109.html:text/html},
}

@inproceedings{zhou_survey_2021,
	address = {Kraków, Poland},
	title = {A {Survey} of {Advances} in {Evolutionary} {Neural} {Architecture} {Search}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72818-393-0},
	url = {https://ieeexplore.ieee.org/document/9504890/},
	doi = {10.1109/CEC45853.2021.9504890},
	urldate = {2024-11-20},
	booktitle = {2021 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Zhou, Xun and Qin, A. K. and Sun, Yanan and Tan, Kay Chen},
	month = jun,
	year = {2021},
	pages = {950--957},
}

@misc{elsken_neural_2019,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	shorttitle = {Neural {Architecture} {Search}},
	url = {http://arxiv.org/abs/1808.05377},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = apr,
	year = {2019},
	note = {arXiv:1808.05377 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/TZMJNLRH/Elsken et al. - 2019 - Neural Architecture Search A Survey.pdf:application/pdf},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
	note = {arXiv:2205.14135},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/LWENFDDS/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/RCF4D5F6/2205.html:text/html},
}

@misc{halfon_stay_2024,
	title = {Stay {Tuned}: {An} {Empirical} {Study} of the {Impact} of {Hyperparameters} on {LLM} {Tuning} in {Real}-{World} {Applications}},
	shorttitle = {Stay {Tuned}},
	url = {http://arxiv.org/abs/2407.18990},
	doi = {10.48550/arXiv.2407.18990},
	abstract = {Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of {\textgreater} 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that for both models and tuning methods, exploring only a few HP configurations, as recommended by our analysis, can provide excellent results in practice, making this work a valuable resource for practitioners.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Halfon, Alon and Gretz, Shai and Arviv, Ofir and Spector, Artem and Toledo-Ronen, Orith and Katz, Yoav and Ein-Dor, Liat and Shmueli-Scheuer, Michal and Slonim, Noam},
	month = aug,
	year = {2024},
	note = {arXiv:2407.18990},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/XTHLDEZD/Halfon et al. - 2024 - Stay Tuned An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applicat.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/6TLUV3BP/2407.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/ATD4HEXB/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/PEDE2NBR/2106.html:text/html},
}

@article{raiaan_review_2024,
	title = {A {Review} on {Large} {Language} {Models}: {Architectures}, {Applications}, {Taxonomies}, {Open} {Issues} and {Challenges}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {A {Review} on {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/document/10433480/},
	doi = {10.1109/ACCESS.2024.3365742},
	urldate = {2024-11-20},
	journal = {IEEE Access},
	author = {Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
	year = {2024},
	pages = {26839--26874},
}

@misc{noauthor_taking_nodate,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7352306},
	urldate = {2024-11-20},
	keywords = {BO, Review},
	file = {Taking the Human Out of the Loop\: A Review of Bayesian Optimization | IEEE Journals & Magazine | IEEE Xplore:/home/jan/snap/zotero-snap/common/Zotero/storage/7BMDZTLS/7352306.html:text/html},
}

@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	volume = {24},
	url = {https://inria.hal.science/hal-00642998},
	abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap- proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos- sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu- ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex- pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli- able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {Neural Information Processing Systems Foundation},
	author = {Bergstra, James and Bardenet, R. and Bengio, Yoshua and Kégl, Balázs},
	month = dec,
	year = {2011},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/WZXI4XMK/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf:application/pdf},
}

@misc{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	doi = {10.48550/arXiv.1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv:1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Gradient, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/NYG53SWH/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/ZH8VVIRV/1806.html:text/html},
}

@inproceedings{falkner_bohb_2018,
	title = {{BOHB}: {Robust} and {Efficient} {Hyperparameter} {Optimization} at {Scale}},
	shorttitle = {{BOHB}},
	url = {https://proceedings.mlr.press/v80/falkner18a.html},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	language = {en},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1437--1446},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/SP52B88H/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimization at Scale.pdf:application/pdf;Supplementary PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/4FDU26FE/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimization at Scale.pdf:application/pdf},
}

@misc{han_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	month = sep,
	year = {2024},
	note = {arXiv:2403.14608},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/V8N8L4M7/Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/VGSUTDHA/2403.html:text/html},
}

@misc{hayou_lora_2024,
	title = {{LoRA}+: {Efficient} {Low} {Rank} {Adaptation} of {Large} {Models}},
	shorttitle = {{LoRA}+},
	url = {http://arxiv.org/abs/2402.12354},
	doi = {10.48550/arXiv.2402.12354},
	abstract = {In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA\$+\$. In our extensive experiments, LoRA\$+\$ improves performance (1-2 \${\textbackslash}\%\$ improvements) and finetuning speed (up to \${\textbackslash}sim\$ 2X SpeedUp), at the same computational cost as LoRA.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
	month = jul,
	year = {2024},
	note = {arXiv:2402.12354},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/IBN2BRND/Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/86B4F2PS/2402.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/KYCP6MUH/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/4V7RAKWV/1810.html:text/html},
}

@misc{watanabe_tree-structured_2023,
	title = {Tree-{Structured} {Parzen} {Estimator}: {Understanding} {Its} {Algorithm} {Components} and {Their} {Roles} for {Better} {Empirical} {Performance}},
	shorttitle = {Tree-{Structured} {Parzen} {Estimator}},
	url = {http://arxiv.org/abs/2304.11127},
	doi = {10.48550/arXiv.2304.11127},
	abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Watanabe, Shuhei},
	month = may,
	year = {2023},
	note = {arXiv:2304.11127},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/B872AUGZ/Watanabe - 2023 - Tree-Structured Parzen Estimator Understanding Its Algorithm Components and Their Roles for Better.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/FGZIN7M4/2304.html:text/html},
}

@misc{frazier_tutorial_2018,
	title = {A {Tutorial} on {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1807.02811},
	doi = {10.48550/arXiv.1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Frazier, Peter I.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.02811},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/SFJ6KJJD/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/U4VV3ANZ/1807.html:text/html},
}

@article{karl_multi-objective_2023,
	title = {Multi-{Objective} {Hyperparameter} {Optimization} in {Machine} {Learning}—{An} {Overview}},
	volume = {3},
	url = {https://dl.acm.org/doi/10.1145/3610536},
	doi = {10.1145/3610536},
	abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.},
	number = {4},
	urldate = {2024-11-20},
	journal = {ACM Trans. Evol. Learn. Optim.},
	author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
	month = dec,
	year = {2023},
	pages = {16:1--16:50},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/R27E5ZLT/Karl et al. - 2023 - Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview.pdf:application/pdf},
}

@article{merrill_empirical_2021,
	title = {An {Empirical} {Study} of {Bayesian} {Optimization}: {Acquisition} {Versus} {Partition}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {An {Empirical} {Study} of {Bayesian} {Optimization}},
	url = {http://jmlr.org/papers/v22/18-220.html},
	abstract = {Bayesian optimization (BO) is a popular framework for black-box optimization. Two classes of BO approaches have shown promising empirical performance while providing strong theoretical guarantees. The first class optimizes an acquisition function to select points, which is typically computationally expensive and can only be done approximately. The second class of algorithms use systematic space partitioning, which is much cheaper computationally but the selection is typically less informed. This points to a potential trade-off between the computational complexity and empirical performance of these algorithms. The current literature, however, only provides a sparse sampling of empirical comparison points, giving little insight into this trade-off. The primary contribution of this work is to conduct a comprehensive, repeatable evaluation within a common software framework, which we provide as an open-source package. Our results give strong evidence about the relative performance of these methods and reveal a consistent top performer, even when accounting for overall computation time.},
	number = {4},
	urldate = {2024-11-20},
	journal = {Journal of Machine Learning Research},
	author = {Merrill, Erich and Fern, Alan and Fern, Xiaoli and Dolatnia, Nima},
	year = {2021},
	pages = {1--25},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/53UPV62D/Merrill et al. - 2021 - An Empirical Study of Bayesian Optimization Acquisition Versus Partition.pdf:application/pdf;Source Code:/home/jan/snap/zotero-snap/common/Zotero/storage/C73L9W5Q/opt_cmp.html:text/html},
}

@article{munos_optimistic_nodate,
	title = {Optimistic {Optimization} of a {Deterministic} {Function} without the {Knowledge} of its {Smoothness}},
	abstract = {We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.},
	language = {en},
	author = {Munos, Rémi},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/629VFG54/Munos - Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness.pdf:application/pdf},
}

@article{santoni_comparison_2024,
	title = {Comparison of {High}-{Dimensional} {Bayesian} {Optimization} {Algorithms} on {BBOB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3670683},
	doi = {10.1145/3670683},
	abstract = {Bayesian Optimization (BO) is a class of surrogate-based black-box optimization heuristics designed to efficiently locate high-quality solutions for problems that are expensive to evaluate, and therefore allow only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 decision variables. Although many new algorithms have been proposed to address this, it remains unclear which one is best suited for a specific optimization problem. In this work, we compare five state-of-the-art high-dimensional BO algorithms with vanilla BO, CMA-ES, and random search on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the superiority of BO over CMA-ES for limited evaluation budgets and suggest that the most promising approach to improve BO is the use of trust regions. However, we also observe significant performance differences for different function landscapes and budget exploitation phases, indicating improvement potential, e.g., through hybridization of algorithmic components.},
	number = {3},
	urldate = {2024-11-20},
	journal = {ACM Trans. Evol. Learn. Optim.},
	author = {Santoni, Maria Laura and Raponi, Elena and Leone, Renato De and Doerr, Carola},
	month = jul,
	year = {2024},
	pages = {17:1--17:33},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/9XAEFQZ9/Santoni et al. - 2024 - Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB.pdf:application/pdf},
}

@article{folch_combining_2023,
	title = {Combining multi-fidelity modelling and asynchronous batch {Bayesian} {Optimization}},
	volume = {172},
	issn = {0098-1354},
	url = {https://www.sciencedirect.com/science/article/pii/S0098135423000637},
	doi = {10.1016/j.compchemeng.2023.108194},
	abstract = {Bayesian Optimization is a useful tool for experiment design. Unfortunately, the classical, sequential setting of Bayesian Optimization does not translate well into laboratory experiments, for instance battery design, where measurements may come from different sources and their evaluations may require significant waiting times. Multi-fidelity Bayesian Optimization addresses the setting with measurements from different sources. Asynchronous batch Bayesian Optimization provides a framework to select new experiments before the results of the prior experiments are revealed. This paper proposes an algorithm combining multi-fidelity and asynchronous batch methods. We empirically study the algorithm behaviour, and show it can outperform single-fidelity batch methods and multi-fidelity sequential methods. As an application, we consider designing electrode materials for optimal performance in pouch cells using experiments with coin cells to approximate battery performance.},
	urldate = {2024-11-20},
	journal = {Computers \& Chemical Engineering},
	author = {Folch, Jose Pablo and Lee, Robert M. and Shafei, Behrang and Walz, David and Tsay, Calvin and van der Wilk, Mark and Misener, Ruth},
	month = apr,
	year = {2023},
	keywords = {Asynchronous, Batch optimization, Bayesian Optimization, Machine learning, multi-fidelity, Multi-fidelity},
	pages = {108194},
	file = {ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/DUXDN7S7/S0098135423000637.html:text/html;Submitted Version:/home/jan/snap/zotero-snap/common/Zotero/storage/YS4B45EK/Folch et al. - 2023 - Combining multi-fidelity modelling and asynchronous batch Bayesian Optimization.pdf:application/pdf},
}

@article{talbi_metaheuristics_2024,
	title = {Metaheuristics for variable-size mixed optimization problems: {A} unified taxonomy and survey},
	volume = {89},
	issn = {2210-6502},
	shorttitle = {Metaheuristics for variable-size mixed optimization problems},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650224001809},
	doi = {10.1016/j.swevo.2024.101642},
	abstract = {Many real world optimization problems are formulated as mixed-variable optimization problems (MVOPs) which involve both continuous and discrete variables. MVOPs including dimensional variables are characterized by a variable-size search space. Depending on the values of dimensional variables, the number and type of the variables of the problem can vary dynamically. MVOPs and variable-size MVOPs (VMVOPs) are difficult to solve and raise a number of scientific challenges in the design of metaheuristics. Standard metaheuristics have been first designed to address continuous or discrete optimization problems, and are not able to tackle VMVOPs in an efficient way. The development of metaheuristics for solving such problems has attracted the attention of many researchers and is increasingly popular. However, to our knowledge there is no well established taxonomy or comprehensive survey for handling this important family of optimization problems. This paper presents an unified taxonomy for metaheuristic solutions for solving VMVOPs in an attempt to provide a common terminology and classification mechanisms. It provides a general mathematical formulation and concepts of VMVOPs, and identifies the various solving methodologies than can be applied in metaheuristics. The advantages, the weaknesses and the limitations of the presented methodologies are discussed. The proposed taxonomy also allows to identify some open research issues which needs further in-depth investigations.},
	urldate = {2024-11-20},
	journal = {Swarm and Evolutionary Computation},
	author = {Talbi, El-Ghazali},
	month = aug,
	year = {2024},
	keywords = {Decomposition-based optimization, Metaheuristics, Mixed optimization, Mixed variable programming, Mixed-variable optimization problem, Variable-size mixed-variable optimization problem, Variable-space design},
	pages = {101642},
	file = {ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/BFLUB5AD/S2210650224001809.html:text/html;Submitted Version:/home/jan/snap/zotero-snap/common/Zotero/storage/HJKKM6H7/Talbi - 2024 - Metaheuristics for variable-size mixed optimization problems A unified taxonomy and survey.pdf:application/pdf},
}