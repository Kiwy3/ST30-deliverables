

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	doi = {10.48550/arXiv.2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, dataset, mmlu},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
}

@misc{zellers_hellaswag_2019,
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {http://arxiv.org/abs/1905.07830},
	doi = {10.48550/arXiv.1905.07830},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	month = may,
	year = {2019},
	note = {arXiv:1905.07830 [cs]},
	keywords = {Computer Science - Computation and Language, dataset, hellaswag},
	annote = {Comment: ACL 2019. Project page at https://rowanzellers.com/hellaswag},
}


@inproceedings{krogh_simple_1991,
	title = {A {Simple} {Weight} {Decay} {Can} {Improve} {Generalization}},
	volume = {4},
	url = {https://papers.nips.cc/paper_files/paper/1991/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html},
	abstract = {It has  been observed  in  numerical simulations that a weight decay  can  im(cid:173) prove generalization in a feed-forward  neural network.  This paper explains  why.  It is  proven  that  a  weight  decay  has  two effects  in  a  linear  network.  First,  it  suppresses  any  irrelevant  components  of  the  weight  vector  by  choosing  the smallest  vector  that solves  the learning  problem.  Second,  if  the size  is chosen  right,  a weight  decay  can suppress some of the effects  of  static  noise  on  the  targets,  which  improves  generalization  quite  a  lot.  It  is  then  shown  how  to extend  these  results  to networks  with hidden  layers  and  non-linear  units.  Finally  the  theory  is  confirmed  by  some numerical  simulations using  the  data from  NetTalk.},
	urldate = {2025-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Krogh, Anders and Hertz, John},
	year = {1991},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/IVNHFSMH/Krogh et Hertz - 1991 - A Simple Weight Decay Can Improve Generalization.pdf:application/pdf},
}

@misc{hashimoto_stanford_2024,
	title = {Stanford {Alpaca}: {An} {Instruction}-following {LLaMA} model},
	copyright = {Apache-2.0},
	url = {https://github.com/tatsu-lab/stanford_alpaca},
	abstract = {Code and documentation to train Stanford's Alpaca models, and generate the data.},
	urldate = {2024-12-18},
	publisher = {GitHub},
	author = {Hashimoto, Rohan Taori {and} Ishaan Gulrajani {and} Tianyi Zhang {and} Yann Dubois {and} Xuechen Li {and} Carlos Guestrin {and} Percy Liang {and} Tatsunori B.},
	month = dec,
	year = {2024},
	note = {publisher : GitHub},
	keywords = {deep-learning, instruction-following, language-model},
}


@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/KP2GY5KA/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/RN64W3VQ/2302.html:text/html},
}


@misc{jin_time-llm_2024,
	title = {Time-{LLM}: {Time} {Series} {Forecasting} by {Reprogramming} {Large} {Language} {Models}},
	shorttitle = {Time-{LLM}},
	url = {http://arxiv.org/abs/2310.01728},
	doi = {10.48550/arXiv.2310.01728},
	abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y. and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},
	month = jan,
	year = {2024},
	note = {arXiv:2310.01728 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted by the 12th International Conference on Learning Representations (ICLR 2024)},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/7NKCA7C2/Jin et al. - 2024 - Time-LLM Time Series Forecasting by Reprogramming Large Language Models.pdf:application/pdf},
}


@misc{noauthor_what_nodate,
	title = {What is quality control ({QC})?},
	url = {https://www.techtarget.com/whatis/definition/quality-control-QC},
	abstract = {Discover what quality control is and how it works. Also, learn how it relates to and works with quality assurance (QA).},
	language = {en},
	urldate = {2024-12-18},
	journal = {WhatIs},
	file = {Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/AUVIV68B/quality-control-QC.html:text/html},
}


@incollection{kacprzyk_parallel_2015,
	address = {Berlin, Heidelberg},
	title = {Parallel {Evolutionary} {Combinatorial} {Optimization}},
	isbn = {978-3-662-43504-5 978-3-662-43505-2},
	url = {http://link.springer.com/10.1007/978-3-662-43505-2_55},
	language = {en},
	urldate = {2024-11-20},
	booktitle = {Springer {Handbook} of {Computational} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Talbi, El-Ghazali},
	editor = {Kacprzyk, Janusz and Pedrycz, Witold},
	year = {2015},
	doi = {10.1007/978-3-662-43505-2_55},
	keywords = {EA, Parallel},
	pages = {1107--1125},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/4I8B6WJM/Talbi - 2015 - Parallel Evolutionary Combinatorial Optimization.pdf:application/pdf},
}


@article{shahriari_taking_2016,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
	volume = {104},
	issn = {1558-2256},
	shorttitle = {Taking the {Human} {Out} of the {Loop}},
	url = {https://ieeexplore.ieee.org/abstract/document/7352306/authors#authors},
	doi = {10.1109/JPROC.2015.2494218},
	abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
	number = {1},
	urldate = {2024-12-13},
	journal = {Proceedings of the IEEE},
	author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
	month = jan,
	year = {2016},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Bayes methods, Big data, decision making, Decision making, design of experiments, Design of experiments, Genomes, genomic medicine, Linear programming, optimization, Optimization, response surface methodology, Statistical analysis, statistical learning},
	pages = {148--175},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/NZSM4Q4R/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:application/pdf},
}

@inproceedings{firmin_comparative_2023,
	address = {Cham},
	title = {A {Comparative} {Study} of {Fractal}-{Based} {Decomposition} {Optimization}},
	isbn = {978-3-031-34020-8},
	doi = {10.1007/978-3-031-34020-8_1},
	abstract = {In this work, we present a comparative study of 24 different and unique decomposition-based algorithms derived from Fractal Decomposition Algorithm and Simultaneous Optimistic Optimization. These algorithms were built within a generic, flexible and unified algorithmic framework named fractal-based decomposition algorithms. This generic framework is issued from previous works and is succinctly described in this paper. A software, called Zellij, based on this methodology was used to instantiate the 24 algorithms. Under our generic framework, fractal-based decomposition algorithms are made of five independent and well-defined search components: a type of fractal, a tree search algorithm, a scoring method, an exploration, and exploitation strategies. This new family of algorithms, hierarchically decomposes an initial search space using a generic geometrical object, named fractal. The decomposition forms a rooted tree, where fractals are nodes, and the root corresponds to the initial search space. The tree is explored, exploited and expanded using the four other search components. The proposed algorithms were tested and compared to each other on the CEC2020 benchmark. Obtained performances emphasize the impact of each search component, and pointed out the scalability capacity of certain algorithms. Our results strongly suggest that some search components have major impact on FDA and SOO-based algorithms for large-scale problems, whereas others are used to fine tune performances in terms of convergence.},
	language = {en},
	booktitle = {Optimization and {Learning}},
	publisher = {Springer Nature Switzerland},
	author = {Firmin, T. and Talbi, E-G.},
	editor = {Dorronsoro, Bernabé and Chicano, Francisco and Danoy, Gregoire and Talbi, El-Ghazali},
	year = {2023},
	pages = {3--20},
}

@inproceedings{munos_optimistic_2011,
	title = {Optimistic {Optimization} of a {Deterministic} {Function} without the {Knowledge} of its {Smoothness}},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper/2011/hash/7e889fb76e0e07c11733550f2a6c7a5a-Abstract.html},
	abstract = {We consider a global optimization problem of a deterministic function f in a semimetric space, given a finite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric . We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A first contribution is an algorithm, DOO, that requires the knowledge of . We report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then define a second algorithm, SOO, which does not require the knowledge of the semimetric  under which f is smooth, and whose performance is almost as good as DOO optimally-fitted.},
	urldate = {2024-12-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Munos, Rémi},
	year = {2011},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/ZVDAL6B2/Munos - 2011 - Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness.pdf:application/pdf},
}


@article{jones_lipschitzian_1993,
	title = {Lipschitzian optimization without the {Lipschitz} constant},
	volume = {79},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/BF00941892},
	doi = {10.1007/BF00941892},
	abstract = {We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods.},
	language = {en},
	number = {1},
	urldate = {2024-12-18},
	journal = {Journal of Optimization Theory and Applications},
	author = {Jones, D. R. and Perttunen, C. D. and Stuckman, B. E.},
	month = oct,
	year = {1993},
	keywords = {Global optimization, Lipschitzian optimization, space covering, space partitioning},
	pages = {157--181},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/TE6DU8EI/Jones et al. - 1993 - Lipschitzian optimization without the Lipschitz constant.pdf:application/pdf},
}

@article{nakib_deterministic_2017,
	title = {Deterministic metaheuristic based on fractal decomposition for large-scale optimization},
	volume = {61},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494617304623},
	doi = {10.1016/j.asoc.2017.07.042},
	abstract = {In this work a new method based on geometric fractal decomposition to solve large-scale continuous optimization problems is proposed. It consists of dividing the feasible search space into sub-regions with the same geometrical pattern. At each iteration, the most promising ones are selected and further decomposed. This approach tends to provide a dense set of samples and has interesting theoretical convergence properties. Under some assumptions, this approach covers all the search space only in case of small dimensionality problems. The aim of this work is to propose a new algorithm based on this approach with low complexity and which performs well in case of large-scale problems. To do so, a low complex method that profits from fractals properties is proposed. Then, a deterministic optimization procedure is proposed using a single solution-based metaheuristic which is exposed to illustrate the performance of this strategy. Obtained results on common test functions were compared to those of algorithms from the literature and proved the efficiency of the proposed algorithm.},
	urldate = {2024-11-20},
	journal = {Applied Soft Computing},
	author = {Nakib, A. and Ouchraa, S. and Shvai, N. and Souquet, L. and Talbi, E. -G.},
	month = dec,
	year = {2017},
	keywords = {FDA, Geometric fractal decomposition, Large-scale optimization, Local search continuous optimization, Metaheuristics, PBO},
	pages = {468--485},
	file = {ScienceDirect Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/PDY5IXJZ/Nakib et al. - 2017 - Deterministic metaheuristic based on fractal decomposition for large-scale optimization.pdf:application/pdf;ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/H9SW3PX6/S1568494617304623.html:text/html},
}

@incollection{feurer_hyperparameter_2019,
	address = {Cham},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	pages = {3--33},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/9DDQHHXG/Feurer et Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@misc{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	doi = {10.48550/arXiv.1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv:1806.09055},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Gradient},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/NYG53SWH/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/ZH8VVIRV/1806.html:text/html},
}

@misc{bang_multitask_2023,
	title = {A {Multitask}, {Multilingual}, {Multimodal} {Evaluation} of {ChatGPT} on {Reasoning}, {Hallucination}, and {Interactivity}},
	url = {http://arxiv.org/abs/2302.04023},
	doi = {10.48550/arXiv.2302.04023},
	abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	month = nov,
	year = {2023},
	note = {arXiv:2302.04023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 45 pages, AACL 2023},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/6XUR6PJG/Bang et al. - 2023 - A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interac.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2024-12-17},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	keywords = {T5},
	pages = {1--67},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/TH22IINE/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:application/pdf;Source Code:/home/jan/snap/zotero-snap/common/Zotero/storage/Z52T2PK4/text-to-text-transfer-transformer.html:text/html},
}

@inproceedings{lewis_bart_2020,
	address = {Online},
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.703},
	doi = {10.18653/v1/2020.acl-main.703},
	language = {en},
	urldate = {2024-12-17},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	year = {2020},
	pages = {7871--7880},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/86J47TD3/Lewis et al. - 2020 - BART Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and.pdf:application/pdf},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/ASF9A62K/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf},
}


@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/AGELEQGK/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf},
}

@article{raiaan_review_2024,
	title = {A {Review} on {Large} {Language} {Models}: {Architectures}, {Applications}, {Taxonomies}, {Open} {Issues} and {Challenges}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {A {Review} on {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/document/10433480/},
	doi = {10.1109/ACCESS.2024.3365742},
	urldate = {2024-12-17},
	journal = {IEEE Access},
	author = {Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
	year = {2024},
	pages = {26839--26874},
}


@misc{han_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	month = sep,
	year = {2024},
	note = {arXiv:2403.14608 [cs]},
	keywords = {Computer Science - Machine Learning, PEFT},
	annote = {Comment: 25 pages, 12 figures. Due to word limit, the abstract here is truncated. The full abstract is available in the PDF},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/FBITC7M4/Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey.pdf:application/pdf},
}


@misc{wu_googles_2016,
	title = {Google's {Neural} {Machine} {Translation} {System}: {Bridging} the {Gap} between {Human} and {Machine} {Translation}},
	shorttitle = {Google's {Neural} {Machine} {Translation} {System}},
	url = {http://arxiv.org/abs/1609.08144},
	doi = {10.48550/arXiv.1609.08144},
	abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, Łukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	month = oct,
	year = {2016},
	note = {arXiv:1609.08144},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/DLAFU9TI/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/TXDRRXM9/1609.html:text/html},
}

@article{goldberg_primer_2016,
	title = {A {Primer} on {Neural} {Network} {Models} for {Natural} {Language} {Processing}},
	volume = {57},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11030},
	doi = {10.1613/jair.4992},
	abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
	language = {en},
	urldate = {2024-12-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Goldberg, Yoav},
	month = nov,
	year = {2016},
	pages = {345--420},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/7F9AT57R/Goldberg - 2016 - A Primer on Neural Network Models for Natural Language Processing.pdf:application/pdf},
}

@article{khosravi_comprehensive_2011,
	title = {Comprehensive {Review} of {Neural} {Network}-{Based} {Prediction} {Intervals} and {New} {Advances}},
	volume = {22},
	issn = {1941-0093},
	url = {https://ieeexplore.ieee.org/abstract/document/5966350},
	doi = {10.1109/TNN.2011.2162110},
	abstract = {This paper evaluates the four leading techniques proposed in the literature for construction of prediction intervals (PIs) for neural network point forecasts. The delta, Bayesian, bootstrap, and mean-variance estimation (MVE) methods are reviewed and their performance for generating high-quality PIs is compared. PI-based measures are proposed and applied for the objective and quantitative assessment of each method's performance. A selection of 12 synthetic and real-world case studies is used to examine each method's performance for PI construction. The comparison is performed on the basis of the quality of generated PIs, the repeatability of the results, the computational requirements and the PIs variability with regard to the data uncertainty. The obtained results in this paper indicate that: 1) the delta and Bayesian methods are the best in terms of quality and repeatability, and 2) the MVE and bootstrap methods are the best in terms of low computational load and the width variability of PIs. This paper also introduces the concept of combinations of PIs, and proposes a new method for generating combined PIs using the traditional PIs. Genetic algorithm is applied for adjusting the combiner parameters through minimization of a PI-based cost function subject to two sets of restrictions. It is shown that the quality of PIs produced by the combiners is dramatically better than the quality of PIs obtained from each individual method.},
	number = {9},
	urldate = {2024-12-17},
	journal = {IEEE Transactions on Neural Networks},
	author = {Khosravi, Abbas and Nahavandi, Saeid and Creighton, Doug and Atiya, Amir F.},
	month = sep,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Artificial neural networks, Bayesian, Bayesian methods, bootstrap, Cost function, delta, Estimation, mean-variance estimation, Minimization, neural network, prediction interval, Training, Uncertainty},
	pages = {1341--1356},
}

@article{zhang_neural_2000,
	title = {Neural networks for classification: a survey},
	volume = {30},
	issn = {1558-2442},
	shorttitle = {Neural networks for classification},
	url = {https://ieeexplore.ieee.org/abstract/document/897072},
	doi = {10.1109/5326.897072},
	abstract = {Classification is one of the most active research and application areas of neural networks. The literature is vast and growing. This paper summarizes some of the most important developments in neural network classification research. Specifically, the issues of posterior probability estimation, the link between neural and conventional classifiers, learning and generalization tradeoff in classification, the feature variable selection, as well as the effect of misclassification costs are examined. Our purpose is to provide a synthesis of the published research in this area and stimulate further research interests and efforts in the identified topics.},
	number = {4},
	urldate = {2024-12-17},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Zhang, G.P.},
	month = nov,
	year = {2000},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	keywords = {Costs, Decision making, Humans, Input variables, Medical diagnosis, Medical diagnostic imaging, Network synthesis, Neural networks, Probability, Speech recognition},
	pages = {451--462},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/MT773BUW/Zhang - 2000 - Neural networks for classification a survey.pdf:application/pdf},
}

@article{rawat_deep_2017,
	title = {Deep {Convolutional} {Neural} {Networks} for {Image} {Classification}: {A} {Comprehensive} {Review}},
	volume = {29},
	issn = {0899-7667},
	shorttitle = {Deep {Convolutional} {Neural} {Networks} for {Image} {Classification}},
	url = {https://ieeexplore.ieee.org/abstract/document/8016501},
	doi = {10.1162/neco_a_00990},
	abstract = {Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.},
	number = {9},
	urldate = {2024-12-17},
	journal = {Neural Computation},
	author = {Rawat, Waseem and Wang, Zenghui},
	month = sep,
	year = {2017},
	note = {Conference Name: Neural Computation},
	pages = {2352--2449},
}

@inproceedings{zhang_generalized_2018,
	title = {Generalized {Cross} {Entropy} {Loss} for {Training} {Deep} {Neural} {Networks} with {Noisy} {Labels}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/f2925f97bc13ad2852a7a551802feea0-Abstract.html},
	abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and large-scale datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
	urldate = {2024-12-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Zhilu and Sabuncu, Mert},
	year = {2018},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/DDL4FN6T/Zhang and Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.pdf:application/pdf},
}

@article{hospedales_meta-learning_2022,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/abstract/document/9428530},
	doi = {10.1109/TPAMI.2021.3079209},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	number = {9},
	urldate = {2024-12-17},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = sep,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, few-shot learning, learning-to-learn, Machine learning algorithms, Meta-learning, neural architecture search, Neural networks, Optimization, Predictive models, Task analysis, Training, transfer learning},
	pages = {5149--5169},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/6QR3PPZE/Hospedales et al. - 2022 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jan/snap/zotero-snap/common/Zotero/storage/N38U3DA5/9428530.html:text/html},
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2024-11-20},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation},
	pages = {115--133},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/8XVZ284S/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervous activity.pdf:application/pdf},
}

@misc{PlanCalcul,
	title = {Plan {Calcul}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://fr.wikipedia.org/w/index.php?title=Plan_Calcul&oldid=218696510},
	abstract = {Le plan Calcul était un plan gouvernemental français lancé en 1966 par le président Charles de Gaulle sur l'impulsion de Michel Debré et d'un groupe de hauts fonctionnaires et d'industriels, destiné à assurer l'autonomie du pays dans les techniques de l'information, et à développer une informatique française, puis européenne dans le cadre d’Unidata.},
	language = {fr},
	urldate = {2024-11-25},
	journal = {Wikipédia},
	month = sep,
	year = {2024},
	note = {Page Version ID: 218696510},
	file = {Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\XN3LVFBB\\Plan_Calcul.html:text/html},
}

@misc{inria_history,
	title = {Notre histoire {\textbar} {Inria}},
	url = {https://www.inria.fr/fr/notre-histoire},
	urldate = {2024-11-25},
	file = {Notre histoire | Inria:C\:\\Users\\Nathan\\Zotero\\storage\\KBY5GBBG\\notre-histoire.html:text/html},
}

@misc{inria_microsoft,
	title = {Inria {Joint} {Center}},
	url = {https://www.microsoft.com/en-us/research/collaboration/inria-joint-centre/},
	abstract = {The Centre's objective is to pursue fundamental, long-term research in formal methods, software security, and the application of computer science research to the sciences.},
	language = {en-US},
	urldate = {2024-11-25},
	journal = {Microsoft Research},
	file = {Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\T2MBL657\\inria-joint-centre.html:text/html},
}

@misc{numpex,
	title = {Numérique pour l’{Exascale} ({NumPEx})},
	url = {https://anr.fr/fr/france-2030/programmes-et-equipements-prioritaires-de-recherche-pepr/numerique-pour-lexascale-numpex/},
	language = {fr},
	urldate = {2024-11-25},
	journal = {Agence nationale de la recherche},
}


@misc{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	url = {http://arxiv.org/abs/1802.03268},
	doi = {10.48550/arXiv.1802.03268},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	urldate = {2024-11-25},
	publisher = {arXiv},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	month = feb,
	year = {2018},
	note = {arXiv:1802.03268},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\CUHM29GB\\Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter Sharing.pdf:application/pdf;Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\36JGKKTC\\1802.html:text/html},
}


@misc{wang_bayesian_2014,
	title = {Bayesian {Multi}-{Scale} {Optimistic} {Optimization}},
	url = {http://arxiv.org/abs/1402.7005},
	abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and Freitas, Nando de},
	month = feb,
	year = {2014},
	note = {arXiv:1402.7005 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/HYMD2UAY/Wang et al. - 2014 - Bayesian Multi-Scale Optimistic Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/TZ6K6S44/1402.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/9LX3T746/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/A46MMFEF/1412.html:text/html},
}

@article{talbi_automated_2021,
	title = {Automated {Design} of {Deep} {Neural} {Networks}: {A} {Survey} and {Unified} {Taxonomy}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Automated {Design} of {Deep} {Neural} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3439730},
	doi = {10.1145/3439730},
	abstract = {In recent years, research in applying optimization approaches in the automatic design of deep neural networks has become increasingly popular. Although various approaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this hot research topic. In this article, we propose a unified way to describe the various optimization algorithms that focus on common and important search components of optimization algorithms: representation, objective function, constraints, initial solution(s), and variation operators. In addition to large-scale search space, the problem is characterized by its variable mixed design space, it is very expensive, and it has multiple blackbox objective functions. Hence, this unified methodology has been extended to advanced optimization approaches, such as surrogate-based, multi-objective, and parallel optimization.},
	number = {2},
	urldate = {2024-11-20},
	journal = {ACM Comput. Surv.},
	author = {Talbi, El-Ghazali},
	month = mar,
	year = {2021},
	pages = {34:1--34:37},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/4HF649T2/Talbi - 2021 - Automated Design of Deep Neural Networks A Survey and Unified Taxonomy.pdf:application/pdf},
}

@unpublished{firmin_fractal-based_2022,
	title = {A fractal-based decomposition framework for continuous optimization},
	url = {https://hal.science/hal-04474444},
	abstract = {In this paper, we propose a generic algorithmic framework which defines a unified view of fractal decomposition algorithms for continuous optimization. Fractals allow building a hierarchical decomposition of the decision space by using a self-similar geometrical object. The proposed generic framework is made of five distinct and independent search components: fractal geometrical object, tree search, scoring, exploration and exploitation. The genericity of the framework allowed the instantiation of popular algorithms from the optimization, machine learning and computational intelligence communities. Moreover, new optimization algorithms can be designed using various strategies of the search components. This shows the modularity of the proposed algorithmic framework. The computational experiments emphasize the behaviors of fractal-based approaches in terms of scalability, robustness, and the balance between exploitation and exploration in the search space. The obtained results show the significance of each search component of the fractal framework, and the necessity to build harder and well-defined benchmarks which can assess the performance of deterministic, axis-aligned and symmetrical decomposition-based algorithms.},
	urldate = {2024-11-20},
	author = {Firmin, Thomas and Talbi, El-Ghazali},
	month = jul,
	year = {2022},
	keywords = {Continuous optimization, Decomposition, Fractal, High-dimensional optimization, Metaheuristic, Tree search},
	file = {HAL PDF Full Text:/home/jan/snap/zotero-snap/common/Zotero/storage/LXP2FJ8Z/Firmin and Talbi - 2022 - A fractal-based decomposition framework for continuous optimization.pdf:application/pdf},
}

@misc{tribes_hyperparameter_2024,
	title = {Hyperparameter {Optimization} for {Large} {Language} {Model} {Instruction}-{Tuning}},
	url = {http://arxiv.org/abs/2312.00949},
	doi = {10.48550/arXiv.2312.00949},
	abstract = {The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the {\textbackslash}nomad algorithm, achieving a boost in performance and human alignment of the tuned model.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Tribes, Christophe and Benarroch-Lelong, Sacha and Lu, Peng and Kobyzev, Ivan},
	month = jan,
	year = {2024},
	note = {arXiv:2312.00949},
	keywords = {Computer Science - Computation and Language, Fine-tuning, HPO, LLM, Mathematics - Optimization and Control},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/LCC9QEWN/Tribes et al. - 2024 - Hyperparameter Optimization for Large Language Model Instruction-Tuning.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/9R9QNLD4/2312.html:text/html},
}

@article{javaheripi_litetransformersearch_2022,
	title = {{LiteTransformerSearch}: {Training}-free {Neural} {Architecture} {Search} for {Efficient} {Language} {Models}},
	volume = {35},
	shorttitle = {{LiteTransformerSearch}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9949e6906be6448230cdba9a4cb2d564-Abstract-Conference.html},
	language = {en},
	urldate = {2024-11-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Javaheripi, Mojan and de Rosa, Gustavo and Mukherjee, Subhabrata and Shah, Shital and Religa, Tomasz and Teodoro Mendes, Caio Cesar and Bubeck, Sebastien and Koushanfar, Farinaz and Dey, Debadeepta},
	month = dec,
	year = {2022},
	keywords = {LLM, NAS},
	pages = {24254--24267},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/XEI96IIU/Javaheripi et al. - 2022 - LiteTransformerSearch Training-free Neural Architecture Search for Efficient Language Models.pdf:application/pdf},
}

@article{klein_structural_2023,
	title = {Structural {Pruning} of {Large} {Language} {Models} via {Neural} {Architecture} {Search}},
	url = {https://openreview.net/forum?id=VAwgL8kPvr},
	abstract = {Large language models (LLMs) mark the state-of-the-art for natural language understanding. However, their large size poses challenges in deploying them for inference in real-world applications, due to significant GPU memory requirements and high inference latency. This paper explores weight-sharing based neural architecture search (NAS) as a form of structural pruning to find sub-parts of the fine-tuned network that optimally trade-off efficiency, for example in terms of model size or latency, and generalization performance. Unlike traditional pruning methods with fixed thresholds, we propose to adopt a multi-objective approach that identifies the Pareto optimal set of sub-networks, allowing for a more flexible and automated compression process. Our NAS approach achieves up to 50\% compression with less than 5\% performance drop for a fine-tuned BERT model on 7 out of 8 text classification tasks.},
	language = {en},
	urldate = {2024-11-20},
	author = {Klein, Aaron and Golebiowski, Jacek and Ma, Xingchen and Perrone, Valerio and Archambeau, Cedric},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/JXYIXK39/Klein et al. - 2023 - Structural Pruning of Large Language Models via Neural Architecture Search.pdf:application/pdf},
}

@book{foster_deep_2024,
	address = {Paris},
	edition = {Illustrated édition},
	title = {Deep learning génératif},
	isbn = {978-2-412-09269-9},
	abstract = {L'intelligence artificielle générative est le sujet en pointe dans le monde des hautes technologies, notamment grâce à ChatGPT. Très bon démarrage de ChatGPT pour les Nuls , avec plus de 2 000 ex. vendus en 3 mois.Ce guide pratique s'adresse aux ingénieurs de l'apprentissage machine ( machine learning) et aux data scientists qui veulent créer des modèles d'apprentissage profond ( deep learning) génératifs en partant de zéro. Après une présentation des bases du deep learning, à vous les architectures de réseaux de neurones les plus sophistiquées ! Auto-encodeurs variationnels, réseaux antagonistes génératifs et systèmes d'entraînement du langage n'auront bien plus de secrets pour vous. Ce livre explore toutes les potentialités du l'IA générative, en matière de texte, de musique ou d'image, ainsi que les perspectives qu'elle ouvre aux entreprises.},
	language = {Français},
	publisher = {First Interactive},
	author = {Foster, David and Maniez, Dominique},
	year = {2024},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-11-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/BIG93IT6/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{zhang_instruction_2024,
	title = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10792},
	doi = {10.48550/arXiv.2308.10792},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT){\textbackslash}footnote\{In this paper, unless specified otherwise, instruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).\}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	month = nov,
	year = {2024},
	note = {arXiv:2308.10792},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/GLBQRRNH/Zhang et al. - 2024 - Instruction Tuning for Large Language Models A Survey.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/HQG7CH56/2308.html:text/html},
}

@misc{noauthor_llama_nodate,
	title = {The {Llama} 3 {Herd} of {Models} {\textbar} {Research} - {AI} at {Meta}},
	url = {https://ai.meta.com/research/publications/the-llama-3-herd-of-models/},
	urldate = {2024-11-20},
}

@misc{wu_evolutionary_2024,
	title = {Evolutionary {Computation} in the {Era} of {Large} {Language} {Model}: {Survey} and {Roadmap}},
	shorttitle = {Evolutionary {Computation} in the {Era} of {Large} {Language} {Model}},
	url = {http://arxiv.org/abs/2401.10034},
	doi = {10.48550/arXiv.2401.10034},
	abstract = {Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wu, Xingyu and Wu, Sheng-hao and Wu, Jibin and Feng, Liang and Tan, Kay Chen},
	month = may,
	year = {2024},
	note = {arXiv:2401.10034},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, EA, LLM, Survey},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/K4C3W7Q3/Wu et al. - 2024 - Evolutionary Computation in the Era of Large Language Model Survey and Roadmap.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/38C9TJK8/2401.html:text/html},
}

@article{gao_autobert-zero_2022,
	title = {{AutoBERT}-{Zero}: {Evolving} {BERT} {Backbone} from {Scratch}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{AutoBERT}-{Zero}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21311},
	doi = {10.1609/aaai.v36i10.21311},
	abstract = {Transformer-based pre-trained language models like BERT and its variants have recently achieved promising performance in various natural language processing (NLP) tasks. However, the conventional paradigm constructs the backbone by purely stacking the manually designed global self-attention layers, introducing inductive bias and thus leads to sub-optimal. In this work, we make the first attempt to automatically discover novel pre-trained language model (PLM) backbone on a flexible search space containing the most fundamental operations from scratch. Specifically, we propose a well-designed search space which (i) contains primitive math operations in the intra-layer level to explore novel attention structures, and (ii) leverages convolution blocks to be the supplementary for attentions in the inter-layer level to better learn local dependency. To enhance the efficiency for finding promising architectures, we propose an Operation-Priority Neural Architecture Search (OP-NAS) algorithm, which optimizes both the search algorithm and evaluation of candidate models. Specifically, we propose Operation-Priority (OP) evolution strategy to facilitate model search via balancing exploration and exploitation. Furthermore, we design a Bi-branch Weight-Sharing (BIWS) training strategy for fast model evaluation. Extensive experiments show that the searched architecture (named AutoBERT-Zero) significantly outperforms BERT and its variants of different model capacities in various downstream tasks, proving the architecture's transfer and scaling abilities. Remarkably, AutoBERT-Zero-base outperforms RoBERTa-base (using much more data) and BERT-large (with much larger model size) by 2.4 and 1.4 higher score on GLUE test set.},
	language = {en},
	number = {10},
	urldate = {2024-11-20},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Gao, Jiahui and Xu, Hang and Shi, Han and Ren, Xiaozhe and Yu, Philip L. H. and Liang, Xiaodan and Jiang, Xin and Li, Zhenguo},
	month = jun,
	year = {2022},
	note = {Number: 10},
	keywords = {LLM, NAS, Speech \& Natural Language Processing (SNLP)},
	pages = {10663--10671},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/57VWRI7L/Gao et al. - 2022 - AutoBERT-Zero Evolving BERT Backbone from Scratch.pdf:application/pdf},
}

@article{liu_survey_2023,
	title = {A {Survey} on {Evolutionary} {Neural} {Architecture} {Search}},
	volume = {34},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/2008.10937},
	doi = {10.1109/TNNLS.2021.3100554},
	abstract = {Deep Neural Networks (DNNs) have achieved great success in many applications. The architectures of DNNs play a crucial role in their performance, which is usually manually designed with rich expertise. However, such a design process is labour intensive because of the trial-and-error process, and also not easy to realize due to the rare expertise in practice. Neural Architecture Search (NAS) is a type of technology that can design the architectures automatically. Among different methods to realize NAS, Evolutionary Computation (EC) methods have recently gained much attention and success. Unfortunately, there has not yet been a comprehensive summary of the EC-based NAS algorithms. This paper reviews over 200 papers of most recent EC-based NAS methods in light of the core components, to systematically discuss their design principles as well as justiﬁcations on the design. Furthermore, current challenges and issues are also discussed to identify future research in this emerging ﬁeld.},
	language = {en},
	number = {2},
	urldate = {2024-11-20},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Liu, Yuqiao and Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G. and Tan, Kay Chen},
	month = feb,
	year = {2023},
	note = {arXiv:2008.10937 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, EA, NAS},
	pages = {550--570},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/K7EK788T/Liu et al. - 2023 - A Survey on Evolutionary Neural Architecture Search.pdf:application/pdf},
}

@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/677M3UYR/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/GP46FG6G/2109.html:text/html},
}

@inproceedings{zhou_survey_2021,
	address = {Kraków, Poland},
	title = {A {Survey} of {Advances} in {Evolutionary} {Neural} {Architecture} {Search}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72818-393-0},
	url = {https://ieeexplore.ieee.org/document/9504890/},
	doi = {10.1109/CEC45853.2021.9504890},
	urldate = {2024-11-20},
	booktitle = {2021 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Zhou, Xun and Qin, A. K. and Sun, Yanan and Tan, Kay Chen},
	month = jun,
	year = {2021},
	pages = {950--957},
}

@misc{elsken_neural_2019,
	title = {Neural {Architecture} {Search}: {A} {Survey}},
	shorttitle = {Neural {Architecture} {Search}},
	url = {http://arxiv.org/abs/1808.05377},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and errorprone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this ﬁeld of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = apr,
	year = {2019},
	note = {arXiv:1808.05377 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/TZMJNLRH/Elsken et al. - 2019 - Neural Architecture Search A Survey.pdf:application/pdf},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
	note = {arXiv:2205.14135},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/LWENFDDS/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact Attention with IO-Awareness.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/RCF4D5F6/2205.html:text/html},
}

@misc{halfon_stay_2024,
	title = {Stay {Tuned}: {An} {Empirical} {Study} of the {Impact} of {Hyperparameters} on {LLM} {Tuning} in {Real}-{World} {Applications}},
	shorttitle = {Stay {Tuned}},
	url = {http://arxiv.org/abs/2407.18990},
	doi = {10.48550/arXiv.2407.18990},
	abstract = {Fine-tuning Large Language Models (LLMs) is an effective method to enhance their performance on downstream tasks. However, choosing the appropriate setting of tuning hyperparameters (HPs) is a labor-intensive and computationally expensive process. Here, we provide recommended HP configurations for practical use-cases that represent a better starting point for practitioners, when considering two SOTA LLMs and two commonly used tuning methods. We describe Coverage-based Search (CBS), a process for ranking HP configurations based on an offline extensive grid search, such that the top ranked configurations collectively provide a practical robust recommendation for a wide range of datasets and domains. We focus our experiments on Llama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a total of {\textgreater} 10,000 tuning experiments. Our results suggest that, in general, Llama-3-8B and LoRA should be preferred, when possible. Moreover, we show that for both models and tuning methods, exploring only a few HP configurations, as recommended by our analysis, can provide excellent results in practice, making this work a valuable resource for practitioners.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Halfon, Alon and Gretz, Shai and Arviv, Ofir and Spector, Artem and Toledo-Ronen, Orith and Katz, Yoav and Ein-Dor, Liat and Shmueli-Scheuer, Michal and Slonim, Noam},
	month = aug,
	year = {2024},
	note = {arXiv:2407.18990},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/XTHLDEZD/Halfon et al. - 2024 - Stay Tuned An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applicat.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/6TLUV3BP/2407.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/ATD4HEXB/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/PEDE2NBR/2106.html:text/html},
}

@article{raiaan_review_2024,
	title = {A {Review} on {Large} {Language} {Models}: {Architectures}, {Applications}, {Taxonomies}, {Open} {Issues} and {Challenges}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {A {Review} on {Large} {Language} {Models}},
	url = {https://ieeexplore.ieee.org/document/10433480/},
	doi = {10.1109/ACCESS.2024.3365742},
	urldate = {2024-11-20},
	journal = {IEEE Access},
	author = {Raiaan, Mohaimenul Azam Khan and Mukta, Md. Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
	year = {2024},
	pages = {26839--26874},
}

@misc{noauthor_taking_nodate,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/7352306},
	urldate = {2024-11-20},
	keywords = {BO, Review},
	file = {Taking the Human Out of the Loop\: A Review of Bayesian Optimization | IEEE Journals & Magazine | IEEE Xplore:/home/jan/snap/zotero-snap/common/Zotero/storage/7BMDZTLS/7352306.html:text/html},
}

@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	volume = {24},
	url = {https://inria.hal.science/hal-00642998},
	abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap- proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos- sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu- ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex- pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli- able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {Neural Information Processing Systems Foundation},
	author = {Bergstra, James and Bardenet, R. and Bengio, Yoshua and Kégl, Balázs},
	month = dec,
	year = {2011},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/WZXI4XMK/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf:application/pdf},
}

@misc{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	doi = {10.48550/arXiv.1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv:1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Gradient, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/NYG53SWH/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/ZH8VVIRV/1806.html:text/html},
}

@inproceedings{falkner_bohb_2018,
	title = {{BOHB}: {Robust} and {Efficient} {Hyperparameter} {Optimization} at {Scale}},
	shorttitle = {{BOHB}},
	url = {https://proceedings.mlr.press/v80/falkner18a.html},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	language = {en},
	urldate = {2024-11-20},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1437--1446},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/SP52B88H/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimization at Scale.pdf:application/pdf;Supplementary PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/4FDU26FE/Falkner et al. - 2018 - BOHB Robust and Efficient Hyperparameter Optimization at Scale.pdf:application/pdf},
}

@misc{han_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	month = sep,
	year = {2024},
	note = {arXiv:2403.14608},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/V8N8L4M7/Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/VGSUTDHA/2403.html:text/html},
}

@misc{hayou_lora_2024,
	title = {{LoRA}+: {Efficient} {Low} {Rank} {Adaptation} of {Large} {Models}},
	shorttitle = {{LoRA}+},
	url = {http://arxiv.org/abs/2402.12354},
	doi = {10.48550/arXiv.2402.12354},
	abstract = {In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA\$+\$. In our extensive experiments, LoRA\$+\$ improves performance (1-2 \${\textbackslash}\%\$ improvements) and finetuning speed (up to \${\textbackslash}sim\$ 2X SpeedUp), at the same computational cost as LoRA.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
	month = jul,
	year = {2024},
	note = {arXiv:2402.12354},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/IBN2BRND/Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/86B4F2PS/2402.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/KYCP6MUH/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/4V7RAKWV/1810.html:text/html},
}

@misc{watanabe_tree-structured_2023,
	title = {Tree-{Structured} {Parzen} {Estimator}: {Understanding} {Its} {Algorithm} {Components} and {Their} {Roles} for {Better} {Empirical} {Performance}},
	shorttitle = {Tree-{Structured} {Parzen} {Estimator}},
	url = {http://arxiv.org/abs/2304.11127},
	doi = {10.48550/arXiv.2304.11127},
	abstract = {Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Watanabe, Shuhei},
	month = may,
	year = {2023},
	note = {arXiv:2304.11127},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/B872AUGZ/Watanabe - 2023 - Tree-Structured Parzen Estimator Understanding Its Algorithm Components and Their Roles for Better.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/FGZIN7M4/2304.html:text/html},
}

@misc{frazier_tutorial_2018,
	title = {A {Tutorial} on {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1807.02811},
	doi = {10.48550/arXiv.1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Frazier, Peter I.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.02811},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/SFJ6KJJD/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/U4VV3ANZ/1807.html:text/html},
}

@article{karl_multi-objective_2023,
	title = {Multi-{Objective} {Hyperparameter} {Optimization} in {Machine} {Learning}—{An} {Overview}},
	volume = {3},
	url = {https://dl.acm.org/doi/10.1145/3610536},
	doi = {10.1145/3610536},
	abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning (ML) workflows. This arises from the fact that ML methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies from the domains of evolutionary algorithms and Bayesian optimization. We illustrate the utility of multi-objective optimization in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability, and robustness.},
	number = {4},
	urldate = {2024-11-20},
	journal = {ACM Trans. Evol. Learn. Optim.},
	author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
	month = dec,
	year = {2023},
	pages = {16:1--16:50},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/R27E5ZLT/Karl et al. - 2023 - Multi-Objective Hyperparameter Optimization in Machine Learning—An Overview.pdf:application/pdf},
}

@article{merrill_empirical_2021,
	title = {An {Empirical} {Study} of {Bayesian} {Optimization}: {Acquisition} {Versus} {Partition}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {An {Empirical} {Study} of {Bayesian} {Optimization}},
	url = {http://jmlr.org/papers/v22/18-220.html},
	abstract = {Bayesian optimization (BO) is a popular framework for black-box optimization. Two classes of BO approaches have shown promising empirical performance while providing strong theoretical guarantees. The first class optimizes an acquisition function to select points, which is typically computationally expensive and can only be done approximately. The second class of algorithms use systematic space partitioning, which is much cheaper computationally but the selection is typically less informed. This points to a potential trade-off between the computational complexity and empirical performance of these algorithms. The current literature, however, only provides a sparse sampling of empirical comparison points, giving little insight into this trade-off. The primary contribution of this work is to conduct a comprehensive, repeatable evaluation within a common software framework, which we provide as an open-source package. Our results give strong evidence about the relative performance of these methods and reveal a consistent top performer, even when accounting for overall computation time.},
	number = {4},
	urldate = {2024-11-20},
	journal = {Journal of Machine Learning Research},
	author = {Merrill, Erich and Fern, Alan and Fern, Xiaoli and Dolatnia, Nima},
	year = {2021},
	pages = {1--25},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/53UPV62D/Merrill et al. - 2021 - An Empirical Study of Bayesian Optimization Acquisition Versus Partition.pdf:application/pdf;Source Code:/home/jan/snap/zotero-snap/common/Zotero/storage/C73L9W5Q/opt_cmp.html:text/html},
}

@article{munos_optimistic_nodate,
	title = {Optimistic {Optimization} of a {Deterministic} {Function} without the {Knowledge} of its {Smoothness}},
	abstract = {We consider a global optimization problem of a deterministic function f in a semimetric space, given a ﬁnite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric ℓ. We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A ﬁrst contribution is an algorithm, DOO, that requires the knowledge of ℓ. We report a ﬁnite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then deﬁne a second algorithm, SOO, which does not require the knowledge of the semimetric ℓ under which f is smooth, and whose performance is almost as good as DOO optimally-ﬁtted.},
	language = {en},
	author = {Munos, Rémi},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/629VFG54/Munos - Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness.pdf:application/pdf},
}

@article{santoni_comparison_2024,
	title = {Comparison of {High}-{Dimensional} {Bayesian} {Optimization} {Algorithms} on {BBOB}},
	volume = {4},
	url = {https://dl.acm.org/doi/10.1145/3670683},
	doi = {10.1145/3670683},
	abstract = {Bayesian Optimization (BO) is a class of surrogate-based black-box optimization heuristics designed to efficiently locate high-quality solutions for problems that are expensive to evaluate, and therefore allow only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 decision variables. Although many new algorithms have been proposed to address this, it remains unclear which one is best suited for a specific optimization problem. In this work, we compare five state-of-the-art high-dimensional BO algorithms with vanilla BO, CMA-ES, and random search on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the superiority of BO over CMA-ES for limited evaluation budgets and suggest that the most promising approach to improve BO is the use of trust regions. However, we also observe significant performance differences for different function landscapes and budget exploitation phases, indicating improvement potential, e.g., through hybridization of algorithmic components.},
	number = {3},
	urldate = {2024-11-20},
	journal = {ACM Trans. Evol. Learn. Optim.},
	author = {Santoni, Maria Laura and Raponi, Elena and Leone, Renato De and Doerr, Carola},
	month = jul,
	year = {2024},
	pages = {17:1--17:33},
	file = {Full Text PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/9XAEFQZ9/Santoni et al. - 2024 - Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB.pdf:application/pdf},
}

@article{folch_combining_2023,
	title = {Combining multi-fidelity modelling and asynchronous batch {Bayesian} {Optimization}},
	volume = {172},
	issn = {0098-1354},
	url = {https://www.sciencedirect.com/science/article/pii/S0098135423000637},
	doi = {10.1016/j.compchemeng.2023.108194},
	abstract = {Bayesian Optimization is a useful tool for experiment design. Unfortunately, the classical, sequential setting of Bayesian Optimization does not translate well into laboratory experiments, for instance battery design, where measurements may come from different sources and their evaluations may require significant waiting times. Multi-fidelity Bayesian Optimization addresses the setting with measurements from different sources. Asynchronous batch Bayesian Optimization provides a framework to select new experiments before the results of the prior experiments are revealed. This paper proposes an algorithm combining multi-fidelity and asynchronous batch methods. We empirically study the algorithm behaviour, and show it can outperform single-fidelity batch methods and multi-fidelity sequential methods. As an application, we consider designing electrode materials for optimal performance in pouch cells using experiments with coin cells to approximate battery performance.},
	urldate = {2024-11-20},
	journal = {Computers \& Chemical Engineering},
	author = {Folch, Jose Pablo and Lee, Robert M. and Shafei, Behrang and Walz, David and Tsay, Calvin and van der Wilk, Mark and Misener, Ruth},
	month = apr,
	year = {2023},
	keywords = {Asynchronous, Batch optimization, Bayesian Optimization, Machine learning, multi-fidelity, Multi-fidelity},
	pages = {108194},
	file = {ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/DUXDN7S7/S0098135423000637.html:text/html;Submitted Version:/home/jan/snap/zotero-snap/common/Zotero/storage/YS4B45EK/Folch et al. - 2023 - Combining multi-fidelity modelling and asynchronous batch Bayesian Optimization.pdf:application/pdf},
}

@article{talbi_metaheuristics_2024,
	title = {Metaheuristics for variable-size mixed optimization problems: {A} unified taxonomy and survey},
	volume = {89},
	issn = {2210-6502},
	shorttitle = {Metaheuristics for variable-size mixed optimization problems},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650224001809},
	doi = {10.1016/j.swevo.2024.101642},
	abstract = {Many real world optimization problems are formulated as mixed-variable optimization problems (MVOPs) which involve both continuous and discrete variables. MVOPs including dimensional variables are characterized by a variable-size search space. Depending on the values of dimensional variables, the number and type of the variables of the problem can vary dynamically. MVOPs and variable-size MVOPs (VMVOPs) are difficult to solve and raise a number of scientific challenges in the design of metaheuristics. Standard metaheuristics have been first designed to address continuous or discrete optimization problems, and are not able to tackle VMVOPs in an efficient way. The development of metaheuristics for solving such problems has attracted the attention of many researchers and is increasingly popular. However, to our knowledge there is no well established taxonomy or comprehensive survey for handling this important family of optimization problems. This paper presents an unified taxonomy for metaheuristic solutions for solving VMVOPs in an attempt to provide a common terminology and classification mechanisms. It provides a general mathematical formulation and concepts of VMVOPs, and identifies the various solving methodologies than can be applied in metaheuristics. The advantages, the weaknesses and the limitations of the presented methodologies are discussed. The proposed taxonomy also allows to identify some open research issues which needs further in-depth investigations.},
	urldate = {2024-11-20},
	journal = {Swarm and Evolutionary Computation},
	author = {Talbi, El-Ghazali},
	month = aug,
	year = {2024},
	keywords = {Decomposition-based optimization, Metaheuristics, Mixed optimization, Mixed variable programming, Mixed-variable optimization problem, Variable-size mixed-variable optimization problem, Variable-space design},
	pages = {101642},
	file = {ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/BFLUB5AD/S2210650224001809.html:text/html;Submitted Version:/home/jan/snap/zotero-snap/common/Zotero/storage/HJKKM6H7/Talbi - 2024 - Metaheuristics for variable-size mixed optimization problems A unified taxonomy and survey.pdf:application/pdf},
}