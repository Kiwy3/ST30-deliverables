%%-------------------- CHAPTER : Methodology ---------------------%%
\chapter{Methodology}
\label{chap:methodo}
\epigraph{Everyone by now presumably knows about the danger of premature optimization. I think we should be just as worried about premature design - designing too early what a program should do.}{Paul Graham}

The methodology is a cornerstone of any research or project, providing a structured framework to achieve objectives systematically and effectively. It ensures clarity, reproducibility, and reliability by defining the steps, tools, and techniques used to address specific problems. A well-defined methodology not only aligns the research process with its goals but also facilitates critical evaluation by external audiences, allowing them to assess the validity and generalization of the results. In the context of this work, the chosen methodology was pivotal in navigating complex challenges, optimizing processes, and ensuring that outcomes are both credible and relevant.

To ensure my sincere approach, and contribute to open-source domain, all the code of this report is readable on  \href{https://github.com/Kiwy3/}{my github account}\footnote{link : https://github.com/Kiwy3/}. It include every code snippet used in this report (like figure \ref{fig:random_search}), every experiments of chapter \ref{chap:results} and all implementation of \acrshort{hpo}.

In this chapter, I will talk about the contextualization in academic literature, then tackle the elaboration of the blackbox function. The definition of the search space being one of the most crucial step in global optimization, the section \ref{sec:search_space} will focus on this. After this preliminary work, we will enter the core of this report : optimization algorithms. A section about experimental setup, to explore resource and scientific integrity, will precede the conclusion section, approaching insight about the realization of this part.

%%-------------------- SECTION : Literature ---------------------%%

\section{A Literature-Based Approach}
\label{sec:litterature}
In industrial field of works, the goal is to be better than competitor, or at least be better than the past of the company. In research fields, a contribution must aims to be better than existing, at least by one facet. In order to do this, the first step of every research project is to make an exhaustive bibliography of the domain, to understand what's already done, and what could be the contribution of the project. 

Chapter \ref{chap:subject_def} was the result of a first stage of bibliography, to define what's the context of this internship. With this, we have insights and contexts about \acrshort{dnn},\acrshort{llm},\gls{fine_tuning} and \acrshort{peft}, and a first look at global optimization fields. In this chapter, a complementary approach will be done about specific optimization algorithm, frameworks and implementation specific details. 

At the beginning of this internship, I started my bibliography using few articles that my tutor send me, for a first look of the subject. From theses articles, I jumped to referenced articles until I started to make a loop between articles. It allow me to find fundational article like articles \cite{vaswani_attention_2017,talbi_automated_2021}, establishing the core of the domain, and reviews like article \cite{elsken_neural_2019,talbi_automated_2021}, allowing to understand a global context and finding a way to classify what I read before.

To manage my bibliography, in a first time I used Notion App\footnote{\href{https://www.notion.so}{https://www.notion.so}} to make a table for my bibliography, with papers charateristics (title, authors, year ...), an export of bibtex from original site and my notes. The table can be found on this \href{https://ribbon-crown-5f6.notion.site/6539799af4a24b32b6d4b91c4e07de49?v=b1542338391647aaa38cc8bb4ad1d5d8&pvs=4}{link}. When I started writing my article, I thought that it's wasn't pratical to copy bibtex export one by one, and I looked at others tools to manage this. It's how I found \href{https://www.zotero.org/}{Zotero}\footnote{link : \href{https://www.zotero.org/}{https://www.zotero.org/}}, with many options to ease my life like collecting article from web with only one click, and export a collection.

%%-------------------- SECTION : Blackbox Elaboration ---------------------%%
\section{Blackbox Elaboration}
\label{sec:blackbox}
My internship can be seen as global optimization applied to a noisy, mixed-variables, expensive blackbox function. A blackbox function is a process that receive an input (here a set of hyperparameters), and return one (or multiple) value(s) (here the accuracy), without any information about the internal process. 


\begin{figure}[h]
    \centering
    \input{assets/img/chap_3/hpo_workflow}
    \caption{HPO workflow}
    \label{fig:hpo_workflow}
\end{figure}

The blackbox process here is described by figure \ref{fig:hpo_workflow}. This process start by the \gls{fine_tuning} of the model, using training dataset, and then evaluating the model, using the validation dataset. Next sections will explore in details the action box of figure \ref{fig:hpo_workflow}. The reproduction of the blackbox function using Python is done with a \textit{ModelEvaluator} class, reproducing the nexts parts.

%%-------------------- SUBSECTION : Fine Tuning ---------------------%%
\subsection{Fine-Tuning of the Model}
\label{sec:fine_tuning}

For \gls{fine_tuning}, the first step is to choose the model to work with. For this choice, the first element was the kind of tasks we want to work with. For the biggest use case and impact, the focus is done on \textit{Decoder-only} model. Then, based on article \cite{tribes_hyperparameter_2024}, and open-source model availability, I choose to work with a model of LlaMa family.

The LlaMa family, launched on February 24, 2023 with the publication of \say{LLaMA: Open and Efficient Foundation Language Models}\cite{touvron_llama_2023}, is a family of open-source (topology and weights values) \textit{decoder-only} fundational models produced and maintained by Meta AI. Latest releases from september 2024, LlaMa 3\cite{grattafiori_llama_2024} set, include model from one billion of parameters (\textit{LlaMa 3.2-1B}) to 405 billions of parameters (\textit{LLaMA 3.1-405B}), and achieved \acrlong{sota} performances on benchmarks. During the first phase of the elaboration of the fine-tuning, I work with \textit{TinyLlama-1.1B}, a lightweight model based on LlaMa architecture. After this phase, I upgraded to \textit{LlaMa 3.2-3B} for a better fidelity compared to high performance models, but compatible with hardware constraints described in section \ref{sec:exp_setup}.

After the model, the next step is the training dataset. The reference in fine-tuning training dataset is the \textit{Alpaca} dataset\cite{hashimoto_stanford_2024}. It's an AI-generated dataset of 52k examples of instruction-based dialogues from the \textit{Stanford Alpaca} project. The dataset is composed of 3 fields : \textit{input}, \textit{output},\textit{instruction} and \textit{text}. At first, I used \textit{Alpaca-2K} datasets, a small subset of \textit{Alpaca} dataset composed of 2k examples. Then, I used the full \textit{Alpaca} dataset when I reached a stable version. 

For the training of the weights, as described in \ref{sec:dnn}, I use \acrshort{adamw}, a variant of \acrshort{adam} decoupling weigth decay \cite{krogh_simple_1991} from learning rate. Along with the optimizer, the training went with \acrfull{lora} as a \acrfull{peft} methods, as defined in section \ref{sec:fine_tune}. The fine-tuning follow the generic \acrshort{ann} training process, except only \acrshort{lora} are trainable. \acrshort{lora} is applied to all weights inside \acrlong{mha}, i.e. keys, weights, queries and output weights, so the linear layers outside \acrshort{mha} are not affected.

For the implementation, at first I started from example from \gls{lightning} documentation, then I adapted it to my needs. This approach used \gls{pytorch} as backend, providing \textit{LightningModule} and \textit{LightningDataModule} classes. \acrshort{gpt} specific function and classes were implemented in \gls{litgpt} librairy. For loading models, \gls{hf}, the standard hub for model and datasets, is used to manage token with Meta interface. 
After few adaptations, I had python code almost usable for fine-tuning, but the file input and output at each step (after training, merging with \acrshort{lora} weights, conversion for evaluation) was prone to error and file corruption. 

In the last half of December, I decided to restart this part from scratch, using solely \gls{litgpt} library with it's \acrfull{cli}. This approach was easier to implement, and provided a more stable workflow although it reduced the training performance, using another parallel strategy. In this approach, I managed long string corresponding to \acrshort{cli} commands, and I used python \textit{subprocess} to execute them.



\begin{algorithm}[h]
    \caption{Fine-Tuning(model, hyperparameters)}\label{alg:fine_tuning_workflow}
    \KwIn{model, hyperparameters}
    model $ \gets$ load ("LlaMa")\ \Comment*[r]{load \gls{lightning} model using \gls{hf} lib}
    model $\gets$ lora(model, hyperparameters)\ \Comment*[r]{apply \gls{lora} to model using \gls{litgpt} lib}
    $x, y \gets $ load("Alpaca") \Comment*[r]{load dataset using \gls{hf} lib}
    \ 
    \
    \Comment*[l]{automated by \gls{lightning}}
    \ForEach{$(x_i,y_i) \in (x,y)$}{ 
        $\hat y \gets$ model.forward($x$)\ \Comment*[r]{forward pass}
        loss $\gets \mathcal L(\hat{y},y)$\ \Comment*[r]{compute loss}
        model $\gets$ backpropagation(loss)\ \Comment*[r]{backward pass}
    }
    \Return model
\end{algorithm}


Figure \ref{alg:fine_tuning_workflow} summarize the fine-tuning process, to understand global process of this part. The process in taking model and hyperparameters as input, to load model and dataset from \gls{hf} librairy. Then \acrshort{lora} is implemented according to hyperparameters, using \gls{litgpt} librairy. After that, the model is trained using \gls{lightning}, with the classical \acrshort{ann} training process. 


%%-------------------- SUBSECTION : Evaluation ---------------------%%
\subsection{Evaluation of the model}
\label{sec:model_evaluation}

To evaluate an \acrshort{ann}, the standard way is to split the dataset into training and validation datasets. The training dataset is used to train the model, and the validation dataset is used to evaluate the model. The evaluation metric can be the loss, a metric about the difference between the predicted output and the true output, or the accuracy, a metric about the percentage of correct predictions. There exists differents kind of loss, link cross-entropy, or mean-square error, to adapt to the datasets and the problem.

With \acrshort{llm}, the diversity of the tasks, even with a \textit{decoder}-only model, is crucial. During the training, the loss or the accuracy is done with the prediction of the next word, compared to the true one. It does not represent the generalization capability of the model. To deal with it, challenge benchmarks, often using \acrfull{mcq} on diverses thematics, were rising. It's was enhanced by article like \cite{wei_finetuned_2022}, proving the advantage of fine-tuning in terms of generalization. 

Among those challenge benchmark datasets, I choose two of them : one to use during \acrshort{hpo} and the other to look at overfitting. The Hellaswag \cite{zellers_hellaswag_2019} dataset is composed of 40k lines of text and 4 choice of answers, meaning random pick lead to 25\% of accuracy. I use this first during \acrshort{hpo}. The MMLU dataset \cite{hendrycks_measuring_2021} is a dataset over multiples subjects, use to prevent overfitting.

The implementation of this part is done with \gls{litgpt} library, as a \acrshort{cli} like for training. Under the \gls{litgpt} part, it's using lm\_eval library from \gls{hf} to manage the evaluation of the accuracy. 



%%-------------------- SECTION : Search Space ---------------------%%
\section{Search Space Definition}
\label{sec:search_space}

The search space is defined with the choice of hyperparameters, theirs bounds, theirs types and even the scale of theirs steps.  Well-defined search space is crucial for a correct application of \acrshort{hpo} : if the space is too high-dimensional, the \acrshort{hpo} algorithm will need too many shots to converge to a good solution, if it's too small, we are missing it's relevance. 

The search space is composed of 5 hyperparameters, from classical training hyperparameters to \acrshort{lora} specific ones. A detailed presentation of hyperparameters is just below, but one can look at table \ref{tab:hyperparam_table} for a summary.
\begin{itemize}
    \item LoRA rank : with LoRA methods, the fine-tuning weights matrix $\Delta W \in \mathbb{R}^{n*p}$ is replaced by two matrices $A \in \mathbb{R}^{r*p}$ and $B \in \mathbb{R}^{n*r}$ with $\Delta W = B*A$. $r$ is called the LoRA rank, and scale the reduction of the weights matrix. It's an integer, and it's value range from 1 to 512, following article \cite{tribes} indication.
    \item LoRA scale ($\alpha$) : when merging pre-trained weights $W_0$ and Lora fine-tuned weights $B*A$, the scale $\alpha$ is weighting the influence of fine-tuning, with $W = W_0 + \frac{\alpha}{r} * (B*A)$. It's a continuous value, from $1$ to $100$.
    \item Learning rate : the learning rate is a classical hyperparameter used in \acrshort{hpo}, weighting the gradient of each weight when doing backpropagation. It is often tuned in a logarithmic scale, to manage effectively the exploration. 
    \item Dropout probability : based on article \cite{srivastava_dropout_2014}, dropout is a method used to prevent over-fitting, by randomly fixing cells/layers to zeroes during one iteration. Being a probability, it's bounds by 0 and 1.
    \item Weight decay : weight decay is used to improve generalization capacity, as proved in article \cite{krogh_simple_1991}, by reducing the weights at each iterations by a small value, to force the model to use new inputs. Typically, the parameter for weight decay is set on a logarithmic scale between $10^{-3}$ and $10^{-1}$.
\end{itemize}





\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Hyper-parameter}} & \multicolumn{2}{|c|}{\textbf{Optimization range}} & \multirow{2}{*}{\textbf{Conversion}} \\
        \cline{2-3}
         & \textbf{Lower Bound} & \textbf{Upper Bound} &  \\
        \hline
        \textbf{Learning Rate} & $-10$ & $-1$ & $f(x) = 10^{x}$ \\
        \hline
        \textbf{LoRA Rank} & 2 & 32 & $f(x) = \text{round}(x)$ \\
        \hline
        \textbf{LoRA scale ($\alpha$)} & 16 & 64 & $f(x) = \text{round}(x)$ \\
        \hline
        \textbf{LoRA Dropout} & 0 & 0.5 & $f(x) = x$ \\
        \hline
        \textbf{Weight Decay} & $-3$ & $-1$ & $f(x) = 10^{x}$  \\
        \hline
    \end{tabular}
    \caption{Summary of Hyperparameter Search Space}
    \label{tab:hyperparam_table}
\end{table}

For the 2 integers variables (LoRA rank and LoRA scale), to adapt to continuous optimization algorithms, the relax and round methods will be applied. It mean that the integers constraints is relaxed when generating a solution, and is rounded when evaluating a solution. Others methods like computing with lower and upper discrete value can be used, but this one was kept for simplicity and computation costs.
For the 2 variables with logarithmic scale (learning rate and weight decay), to explore with consistency the search space, the optimization algorithm will be bound between the exponential values of the bounds, and a logarithm will be applied when evaluating a solution.

%%-------------------- SECTION : Optimization ---------------------%%
\section{Optimization Algorithms}
\label{sec:opt}
Linked with section \ref{sec : opt_algo}, this part aims to describe the implemented algorithms, and how they are applied to the optimization problem. It start from elements of section \ref{sec : opt_algo}, then describe algorithms and show examples of application.

The first approach to explore is \acrfull{smbo}, and particularly \acrfull{bo} using \acrfull{gp} (\acrshort{bo}-\acrshort{gp}). Then considering the dimensionnality of the problem, and the \acrshort{pbo} performance benchmark in article \cite{firmin_fractal-based_2022}, I went with \acrfull{soo} algorithm as representative of \acrshort{pbo} methods. After theses two approachs, section \ref{sec:bamsoo} present an hybrid approach, combining the intrinsic parallel abilities of \acrshort{pbo} combined to the efficiency and exploitation of \acrshort{bo}.

\subsection{\acrfull{bo}}
\label{sec:bo}

Bayesian Optimization is often defined as a "sequential model-based approach to solving problem" \cite{shahriari_taking_2016}. A surrogate model is used to build a posterior considering a prior knowledge formed on known points. On this posterior, an acquisition function is compute to act as the surrogate function for the function to optimize. On this work, a focus is done on \acrshort{gp} for the \acrshort{bo} surrogate. \acrshort{gp} use the kernel trick to build a bayesian nonparametric regression model. It use a mean vector $m_i$ and a covariance matrix $K_{i,j}$ to define the prior function : 

\begin{equation}
    \text{f} | X \sim  \mathcal N (m,K)
    \label{eq:prior_gp}
\end{equation}

From the prior function and the data points $\mathcal D$, the \acrshort{gp} build a posterior. On this posterior is build an acquisition function used as a surrogate for the objective function.

\begin{algorithm}[h]
    \caption{BO}
    \label{algo:bo}
    \KwIn{$\Omega$, $f$, $K_D$, $\mathcal{O}$, $f_{\text{acq}}$, $n_{\text{init}}$, $n_{\text{opt}}$}
    
    \tcp{initiate function}
    \For{$i \gets 1$ \KwTo $n_{\text{init}}$}{
        $\lambda' \gets \text{LHS}(\Omega, \mathcal{D})$ \tcp{Sample one point}
        $\mathcal{D} \gets \mathcal{D} \cup \{(\lambda', f(\lambda'))\}$ \tcp{Add solution and evaluation to set of data}
    }
    \For{$i \gets 1$ \KwTo $n_{\text{opt}}$}{ 
        $\mu_D, K_D \gets \text{Update}(K_D, \mathcal{D})$ \;
        $K_D \gets \text{Fit}(\text{GP}(K_D), \mathcal{D})$ \;
        $\lambda' \gets \text{Optimize}(f_{\text{acq}}(K_D), \mathcal{O})$ \tcp{Generate new point}
        $\mathcal{D} \gets \mathcal{D} \cup \{(\lambda', f(\lambda'))\}$ \tcp{scoring function}
    }
    
    \Return best of $\{(\lambda^*, f(\lambda^*)) \in \mathcal{D}\}$
\end{algorithm}
    

Algorithm \ref{algo:bo} offer an overview of the BO process. To ease the first build of the surrogate, it's crucial, as proven in article \cite{wilson_efficiently_2020}, to sample efficiently the search space. This sampling provides information for the Gaussian Process to estimation the function. Like article \cite{borisut_adaptive_2023}, \acrfull{lhs} is used as a sampling method, for a defined budget called $n\_init$. 

After this preliminary phase, a second phase is done with on loop containing the update of the Gaussian Process, the optimization of the acquisition function to obtain a new points to evaluate. After the evaluation of the point, the point is added to the history $\mathcal D$ and so on. The loop end based on a budget $n_{opt}$. 

For this algorithm, the first requirements is the search space, and the objective function already described in \ref{sec:search_space} and \ref{sec:obj_fun} respectively. On the \acrshort{gp} part, we need to define a Kernel function $K_\mathcal D$, an acquisition function $f_{acq}$ and an Inner Optimizer $\mathcal O$. The acquisition function is logEI, more reliable than EI, based on article \cite{ament_unexpected_2024}. The kernel and the inner optimizer are the standard implementation of Botorch, introduced in the next paragraph, with a radial basis function kernel and multi-start optimization method. 

BoTorch \cite{balandat_botorch_2020} is a Bayesian Optimization library built on PyTorch, designed for efficient and scalable optimization of expensive black-box functions. Leveraging PyTorch's GPU acceleration and integration with GPyTorch \cite{gardner_gpytorch_2021} for Gaussian Processes, BoTorch enables advanced surrogate modeling and optimization. Botorch is used on this work for all tasks including \acrshort{gp}, this part and section \ref{sec:bamsoo}

\subsection{\acrfull{soo}}
\label{sec:soo}

In global optimization, a lot of methods are based on the partition of the search space \cite{nakib_deterministic_2017,jones_lipschitzian_1993,munos_optimistic_2011}. Theses approaches are mostly deterministic, and enhance intrinsic parallelization ability. For theses methods, the dimensionality of the problem is a key to choose the specific algorithm. With a dimensionality around 5, based on benchmarks at the end of article \cite{firmin_fractal-based_2022}, the \acrfull{soo} \cite{munos_optimistic_2011} algorithm seems a good way to start. 

\begin{algorithm}[h]
    \caption{SOO}
    \label{algo:soo}
    \KwIn{$\Omega$, $f$, $K$, $n_{\text{max}}$}
    \tcp{initiate }
    $x_{0,0} \gets \text{center}(\Omega)$ \;
    $f_{0,0} \gets f(x_{0,0})$ \;
    $\mathcal{T}_1 \gets \{x_{0,0}, f_{0,0}, \Omega\}$ \;
    $n \gets 1$ \;

    \While{$n < n_{\text{max}}$}{
        $\nu_{\text{max}} \gets -\infty$ \;
        \For{$h \gets 0$ \KwTo $\text{depth}(\mathcal{T}_n)$}{
            $j \gets \arg\max_{j \in \{j \mid (h,j) \in L_n\}} f(x_{h,j})$ \tcp{select function}
            \If{$f(x_{h,j}) > \nu_{\text{max}}$}{
                $\Omega_{h+1,j+1}, \dots, \Omega_{h+1,j+K} \gets \text{section}(\Omega_{h,j}, K)$ \;
                \For{$i \gets 1$ \KwTo $K$}{
                    $n \gets n+1$ \;
                    $x_{h+1,j+i} \gets \text{center}(\Omega_{n})$ \;
                    \textcolor{blue}{
                    $f_{h+1,j+i} \gets f(x_{h+1,j+i})$  \tcp{Scoring function}
                    }
                    $\mathcal{T}_n \gets \{(x_{h+1,j+i}, f_{h+1,j+i}, \Omega_{n+1})\}$ \tcp{add\_leaf function
                }
                $\nu_{\text{max}} \gets f_{h,j}$ \;}
            }
        }
    }
    \Return best of $x_{h,j}, f(x_{h,j})$ \;
    \end{algorithm}
    

\acrshort{soo} is a tree-based space partitioning method for black-box optimization, inspired by \acrfull{mcts} methods. \acrshort{soo} is called optimistic since it assume the existence of $ l$ such that $f(x^*)-f(x) \leq l(x,x^*)$ where $x^*$ is the maximizer of $x$.The algorithm partition the space $\Omega$ by building a tree with smaller and smaller sub-space $\Omega_n$. A node $(h,j)$, the node number $j$ of depth $h$, is scored at the center of his space. 

An expanded node have $K$ children, making the tree a $K$-nary tree. $L_n$ is the \textit{open list} of the tree, to avoid expanding the same nodes over and over. At each round, \acrshort{soo} expand a maximum of one node by depth, meaning that each round score a maximum of $depth*(K)$ solution, enhancing the parallel evaluation of the solution. Summary of \acrshort{soo} is present in algorithm \ref{algo:soo}

The original algorithm manage the end of the loop with the $h_{max}(n)$ function, limiting the depth of the tree search. To compare different algorithm, the stopping criterion here is $n_{max}$, the evaluation budget. 

\subsection{\acrfull{bamsoo}}
\label{sec:bamsoo}

\acrfull{smbo} algorithms harness the exploitation of the informations to define a cost-reduce function to optimize. This approach ensure exploitation but have several limitations, including the parallelization difficulties \cite{X,X}. On the other hand, Partition-based approach are massively parallel, but are computation costly in front of very expensive objective function. To overcome both limitations, hybrid methods, using surrogates and space partition, were developed.

In this work, we focus on \acrshort{bamsoo}, a \acrshort{soo} based algorithm (algorithm \ref{algo:bamsoo}). Like \acrshort{soo}, \acrshort{bamsoo} performs a $K$-inary partitionning of the space, using the center of the partition to evaluate. 

\begin{equation}
    \begin{split}
    \mathcal{UCB}(x| \mathcal D_t) = \mu(x|\mathcal D_t) +  B_N * \sigma(x|\mathcal D_t) 
    \\ \text{with } B_N = \sqrt{2 \log (\pi^2 N^2/6 \eta)} , \eta \in (0,1)      
    \end{split}  
    \label{eq:ucb}
\end{equation}

The difference lies primarily in the scoring $g(.)$ of the partitions (lines 25 to 30). In the face of an expensive objective function, \acrshort{bamsoo} leverages a \acrshort{gp} surrogate to estimate the potential of a point, using the \acrshort{ucb} as a measure of expected performance. Given a partition with center $x$ and existing evaluations $\mathcal{D}_t$, the \acrshort{ucb} of $x$, defined in Equation \ref{eq:ucb}, is compared against the best evaluation so far, $f^+$. If the \acrshort{ucb} is higher than $f^+$, the algorithm evaluates $x$ directly using the objective function $f(.)$. Otherwise, the partition is scored using the \acrshort{lcb} of $x$, reflecting the lower bound of potential improvement.


\begin{algorithm}[h]
    \caption{BamSOO scoring}
    \label{algo:bamsoo_scoring}    
                    \If{$\mathcal{UCB}(x_{h+1,j+i}, \mu, \sigma) \geq f^+$}{
                        $g_{h+1,j+i} \gets f(x_{h+1,j+i})$ \;
                        $t \gets t+1$ \;
                    }\Else{
                        $g_{h+1,j+i} \gets \mathcal{LCB}(x_{h+1,j+i}, \mu, \sigma)$ \;
                    }
    
                    \If{$g_{h+1,j+i} > f^+$}{
                        $f^+ \gets g_{h+1,j+i}$ \;
                    }
                    $n \gets n+1$ \;
                    $\mathcal{T}_n \gets \{(x_{h+1,j+i}, f_{h+1,j+i}, \Omega_{h+1,j+i})\}$ \;
                
    \Return best of $x_{h,j}, g(x_{h,j})$ \;
    \end{algorithm}

To sum up, this algorithm efficiently navigates the search space by leveraging partition-based exploration and utilizing the gathered information to prioritize regions with higher potential. By balancing exploration of untested partitions and exploitation of known promising areas, \acrshort{bamsoo} optimally allocates the evaluation budget, ensuring that computational resources are directed toward regions most likely to contain optimal solutions. This hybrid approach significantly mitigates the limitations of traditional methods, combining the global scalability of partitioning with the precision of \acrshort{gp}-based surrogate modeling.

For the implementation of the GP components, including the calculation of \acrshort{lcb} and \acrshort{ucb} scores, the BoTorch library was employed. This choice ensures computational efficiency and robustness, as BoTorch provides a modular framework for Bayesian optimization and GP modeling, seamlessly integrating with the partition-based structure of BamSOO. By adhering to the methodology outlined in section \ref{sec:bo}, the framework ensures consistency in surrogate modeling and acquisition function computation, further enhancing the effectiveness of the algorithm in high-dimensional, continuous search spaces.

\FloatBarrier

%%-------------------- SECTION : Concrete implentation ---------------------%%
\section{Concrete Implementation}
\label{sec:concrete_impl}
To implement what's described in section \ref{sec:blackbox} to \ref{sec:opt}, I used Python as my main language. After a first phase of coding only with function on a small number of file, I started to rethink everything as \acrfull{oop}. When the rework from scratch happened in December, I used this opportunity to structure my code with \acrshort{oop}. 

\begin{figure}[h]
    \centering
    \input{assets/img/chap_3/class_diag.tex}
    \caption{Class diagramm of the optimization framework}
    \label{fig:class_diag}
\end{figure}

Figure \ref{fig:class_diag} is an UML class diagramm presenting the whole framework of my internship, split in tree parts : the optimization part, including all optimization algorithm seen in section \ref{sec:opt}, the search space part, and evaluation part/

At the left, the optimization part includes a base class for optimization algorithm, managing all recurrent tasks, especially scoring and extracting the best result. From this class is built \acrshort{soo} and \acrshort{bo} classes. All theirs attributes and functions are described in section \ref{sec:soo} and \ref{sec:bo} respectively, with comment in algorithms to precise which function is used for which line. \acrshort{soo} being a tree-based algorithm, a leaf class in create to manage the leaf of the tree, and especially the decomposition of the space. From \acrshort{soo} class, \acrshort{bamsoo} is built, add especially the \acrshort{gp} surrogate, and updating the scoring function with \acrshort{ucb} and \acrshort{lcb} to adapt to algorithm \ref{algo:bamsoo_scoring}.

At the top right, the search space part is firstly composed with a search space class. This class is composed on multiple var class, use to automate all function used by each variables. Then, it's used to manage the space, like section to split the space for \acrshort{soo}, or other functions to deal with algorithms and compatibility needs. Then, the Solution class inherit the search space class, to force a solution to be define in a search space, and then manage the conversion of the solution, with the conversion function of table \ref{tab:hyperparam_table}.

The last part is the eval class, use to store the model id, the experiment folder, the task and then call the \textit{train\_and\_eval} function when the scoring function is called. The whole structure allow an easy understanding of the framework, an easier testing and debuging process and a better reusability of each part of the code. 

Along these part, all experiments are also stored as a subpackage of the whole, and then called from the main file. This method ease the reproducibility of the experiments, and avoid import or module error from python package management. 

%%-------------------- SECTION : Experimental Setup ---------------------%%
\section{experiments setup}
\label{sec:exp_setup}

Grid5000, chuc etc.


%%-------------------- SECTION : Difficulties ---------------------%%
\section{Difficulties}
\label{sec:opt_difficulties}
Several challenges were encountered during the development and implementation of this optimization framework. The foremost difficulty is the high computational expense of evaluating LLMs, which necessitates the careful allocation of resources and the adoption of efficient evaluation strategies. Handling mixed-type hyperparameters, particularly the interplay between continuous and discrete variables, posed additional complexities. Existing optimization techniques often struggle with these mixed spaces, requiring innovative solutions such as relaxation and partitioning to ensure convergence. Finally, ensuring the generalizability of the fine-tuning results across different benchmarks and datasets proved challenging, as model performance is highly dependent on task-specific characteristics and dataset quality.