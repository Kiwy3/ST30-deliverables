%%-------------------- CHAPTER : Methodology ---------------------%%
\chapter{Methodology}
\label{chap:methodo}
\epigraph{Everyone by now presumably knows about the danger of premature optimization. I think we should be just as worried about premature design - designing too early what a program should do.}{Paul Graham}

The methodology is a cornerstone of any research or project, providing a structured framework to achieve objectives systematically and effectively. It ensures clarity, reproducibility, and reliability by defining the steps, tools, and techniques used to address specific problems. A well-defined methodology not only aligns the research process with its goals but also facilitates critical evaluation by external audiences, allowing them to assess the validity and generalization of the results. In the context of this work, the chosen methodology was pivotal in navigating complex challenges, optimizing processes, and ensuring that outcomes are both credible and relevant.

To ensure my sincere approach, and contribute to open-source domain, all the code of this report is readable on  \href{https://github.com/Kiwy3/}{my github account}\footnote{link : https://github.com/Kiwy3/}. It include every code snippet used in this report (like figure \ref{fig:random_search}), every experiments of chapter \ref{chap:results} and all implementation of \acrshort{hpo}.

In this chapter, I will talk about the contextualization in academic literature, then tackle the elaboration of the blackbox function. The definition of the search space being one of the most crucial step in global optimization, the section \ref{sec:search_space} will focus on this. After this preliminary work, we will enter the core of this report : optimization algorithms. A section about experimental setup, to explore resource and scientific integrity, will precede the conclusion section, approaching insight about the realization of this part.

%%-------------------- SECTION : Literature ---------------------%%

\section{A Literature-Based Approach}
\label{sec:litterature}
In industrial field of works, the goal is to be better than competitor, or at least be better than the past of the company. In research fields, a contribution must aims to be better than existing, at least by one facet. In order to do this, the first step of every research project is to make an exhaustive bibliography of the domain, to understand what's already done, and what could be the contribution of the project. 

Chapter \ref{chap:subject_def} was the result of a first stage of bibliography, to define what's the context of this internship. With this, we have insights and contexts about \acrshort{dnn},\acrshort{llm},\gls{fine_tuning} and \acrshort{peft}, and a first look at global optimization fields. In this chapter, a complementary approach will be done about specific optimization algorithm, frameworks and implementation specific details. 

At the beginning of this internship, I started my bibliography using few articles that my tutor send me, for a first look of the subject. From theses articles, I jumped to referenced articles until I started to make a loop between articles. It allow me to find fundational article like articles \cite{vaswani_attention_2017,talbi_automated_2021}, establishing the core of the domain, and reviews like article \cite{elsken_neural_2019,talbi_automated_2021}, allowing to understand a global context and finding a way to classify what I read before.

To manage my bibliography, in a first time I used Notion App\footnote{\href{https://www.notion.so}{https://www.notion.so}} to make a table for my bibliography, with papers charateristics (title, authors, year ...), an export of bibtex from original site and my notes. The table can be found on this \href{https://ribbon-crown-5f6.notion.site/6539799af4a24b32b6d4b91c4e07de49?v=b1542338391647aaa38cc8bb4ad1d5d8&pvs=4}{link}. When I started writing my article, I thought that it's wasn't pratical to copy bibtex export one by one, and I looked at others tools to manage this. It's how I found \href{https://www.zotero.org/}{Zotero}\footnote{link : \href{https://www.zotero.org/}{https://www.zotero.org/}}, with many options to ease my life like collecting article from web with only one click, and export a collection.

%%-------------------- SECTION : Blackbox Elaboration ---------------------%%
\section{Blackbox Elaboration}
\label{sec:blackbox}
My internship can be seen as global optimization applied to a noisy, mixed-variables, expensive blackbox function. A blackbox function is a process that receive an input (here a set of hyperparameters), and return one (or multiple) value(s) (here the accuracy), without any information about the internal process. 


\begin{figure}[h]
    \centering
    \input{assets/img/chap_3/hpo_workflow}
    \caption{HPO workflow}
    \label{fig:hpo_workflow}
\end{figure}

The blackbox process here is described by figure \ref{fig:hpo_workflow}. This process start by the \gls{fine_tuning} of the model, using training dataset, and then evaluating the model, using the validation dataset. Next sections will explore in details the action box of figure \ref{fig:hpo_workflow}. The reproduction of the blackbox function using Python is done with a \textit{ModelEvaluator} class, reproducing the nexts parts.

%%-------------------- SUBSECTION : Fine Tuning ---------------------%%
\subsection{Fine-Tuning of the Model}
\label{sec:fine_tuning}

For \gls{fine_tuning}, the first step is to choose the model to work with. For this choice, the first element was the kind of tasks we want to work with. For the biggest use case and impact, the focus is done on \textit{Decoder-only} model. Then, based on article \cite{tribes_hyperparameter_2024}, and open-source model availability, I choose to work with a model of LlaMa family.

The LlaMa family, launched on February 24, 2023 with the publication of \say{LLaMA: Open and Efficient Foundation Language Models}\cite{touvron_llama_2023}, is a family of open-source (topology and weights values) \textit{decoder-only} fundational models produced and maintained by Meta AI. Latest releases from september 2024, LlaMa 3\cite{grattafiori_llama_2024} set, include model from one billion of parameters (\textit{LlaMa 3.2-1B}) to 405 billions of parameters (\textit{LLaMA 3.1-405B}), and achieved \acrlong{sota} performances on benchmarks. During the first phase of the elaboration of the fine-tuning, I work with \textit{TinyLlama-1.1B}, a lightweight model based on LlaMa architecture. After this phase, I upgraded to \textit{LlaMa 3.2-3B} for a better fidelity compared to high performance models, but compatible with hardware constraints described in section \ref{sec:exp_setup}.

After the model, the next step is the training dataset. The reference in fine-tuning training dataset is the \textit{Alpaca} dataset\cite{hashimoto_stanford_2024}. It's an AI-generated dataset of 52k examples of instruction-based dialogues from the \textit{Stanford Alpaca} project. The dataset is composed of 3 fields : \textit{input}, \textit{output},\textit{instruction} and \textit{text}. At first, I used \textit{Alpaca-2K} datasets, a small subset of \textit{Alpaca} dataset composed of 2k examples. Then, I used the full \textit{Alpaca} dataset when I reached a stable version. 

For the training of the weights, as described in \ref{sec:dnn}, I use \acrshort{adamw}, a variant of \acrshort{adam} decoupling weigth decay \cite{krogh_simple_1991} from learning rate. Along with the optimizer, the training went with \acrfull{lora} as a \acrfull{peft} methods, as defined in section \ref{sec:fine_tune}. The fine-tuning follow the generic \acrshort{ann} training process, except only \acrshort{lora} are trainable. \acrshort{lora} is applied to all weights inside \acrlong{mha}, i.e. keys, weights, queries and output weights, so the linear layers outside \acrshort{mha} are not affected.

For the implementation, at first I started from example from \gls{lightning} documentation, then I adapted it to my needs. This approach used \gls{pytorch} as backend, providing \textit{LightningModule} and \textit{LightningDataModule} classes. \acrshort{gpt} specific function and classes were implemented in \gls{litgpt} librairy. For loading models, \gls{hf}, the standard hub for model and datasets, is used to manage token with Meta interface. 
After few adaptations, I had python code almost usable for fine-tuning, but the file input and output at each step (after training, merging with \acrshort{lora} weights, conversion for evaluation) was prone to error and file corruption. 

In the last half of December, I decided to restart this part from scratch, using solely \gls{litgpt} library with it's \acrfull{cli}. This approach was easier to implement, and provided a more stable workflow although it reduced the training performance, using another parallel strategy. In this approach, I managed long string corresponding to \acrshort{cli} commands, and I used python \textit{subprocess} to execute them.



\begin{algorithm}[h]
    \caption{Fine-Tuning(model, hyperparameters)}\label{alg:fine_tuning_workflow}
    \KwIn{model, hyperparameters}
    model $ \gets$ load ("LlaMa")\ \Comment*[r]{load \gls{lightning} model using \gls{hf} lib}
    model $\gets$ lora(model, hyperparameters)\ \Comment*[r]{apply \gls{lora} to model using \gls{litgpt} lib}
    $x, y \gets $ load("Alpaca") \Comment*[r]{load dataset using \gls{hf} lib}
    \ 
    \
    \Comment*[l]{automated by \gls{lightning}}
    \ForEach{$(x_i,y_i) \in (x,y)$}{ 
        $\hat y \gets$ model.forward($x$)\ \Comment*[r]{forward pass}
        loss $\gets \mathcal L(\hat{y},y)$\ \Comment*[r]{compute loss}
        model $\gets$ backpropagation(loss)\ \Comment*[r]{backward pass}
    }
    \Return model
\end{algorithm}


Figure \ref{alg:fine_tuning_workflow} summarize the fine-tuning process, to understand global process of this part. The process in taking model and hyperparameters as input, to load model and dataset from \gls{hf} librairy. Then \acrshort{lora} is implemented according to hyperparameters, using \gls{litgpt} librairy. After that, the model is trained using \gls{lightning}, with the classical \acrshort{ann} training process. 


%%-------------------- SUBSECTION : Evaluation ---------------------%%
\subsection{Evaluation of the model}
\label{sec:model_evaluation}

To evaluate an \acrshort{ann}, the standard way is to split the dataset into training and validation datasets. The training dataset is used to train the model, and the validation dataset is used to evaluate the model. The evaluation metric can be the loss, a metric about the difference between the predicted output and the true output, or the accuracy, a metric about the percentage of correct predictions. There exists differents kind of loss, link cross-entropy, or mean-square error, to adapt to the datasets and the problem.

With \acrshort{llm}, the diversity of the tasks, even with a \textit{decoder}-only model, is crucial. During the training, the loss or the accuracy is done with the prediction of the next word, compared to the true one. It does not represent the generalization capability of the model. To deal with it, challenge benchmarks, often using \acrfull{mcq} on diverses thematics, were rising. It's was enhanced by article like \cite{wei_finetuned_2022}, proving the advantage of fine-tuning in terms of generalization. 

Among those challenge benchmark datasets, I choose two of them : one to use during \acrshort{hpo} and the other to look at overfitting. The Hellaswag \cite{zellers_hellaswag_2019} dataset is composed of 40k lines of text and 4 choice of answers, meaning random pick lead to 25\% of accuracy. I use this first during \acrshort{hpo}. The MMLU dataset \cite{hendrycks_measuring_2021} is a dataset over multiples subjects, use to prevent overfitting.

The implementation of this part is done with \gls{litgpt} library, as a \acrshort{cli} like for training. Under the \gls{litgpt} part, it's using lm\_eval library from \gls{hf} to manage the evaluation of the accuracy. 



%%-------------------- SECTION : Search Space ---------------------%%
\section{Search Space Definition}
\label{sec:search_space}

The search space is defined with the choice of hyperparameters, theirs bounds, theirs types and even the scale of theirs steps.  Well-defined search space is crucial for a correct application of \acrshort{hpo} : if the space is too high-dimensional, the \acrshort{hpo} algorithm will need too many shots to converge to a good solution, if it's too small, we are missing it's relevance. 

The search space is composed of 5 hyperparameters, from classical training hyperparameters to \acrshort{lora} specific ones. A detailed presentation of hyperparameters is just below, but one can look at table \ref{tab:hyperparam_table} for a summary.
\begin{itemize}
    \item LoRA rank : with LoRA methods, the fine-tuning weights matrix $\Delta W \in \mathbb{R}^{n*p}$ is replaced by two matrices $A \in \mathbb{R}^{r*p}$ and $B \in \mathbb{R}^{n*r}$ with $\Delta W = B*A$. $r$ is called the LoRA rank, and scale the reduction of the weights matrix. It's an integer, and it's value range from 1 to 512, following article \cite{tribes} indication.
    \item LoRA scale ($\alpha$) : when merging pre-trained weights $W_0$ and Lora fine-tuned weights $B*A$, the scale $\alpha$ is weighting the influence of fine-tuning, with $W = W_0 + \frac{\alpha}{r} * (B*A)$. It's a continuous value, from $1$ to $100$.
    \item Learning rate : the learning rate is a classical hyperparameter used in \acrshort{hpo}, weighting the gradient of each weight when doing backpropagation. It is often tuned in a logarithmic scale, to manage effectively the exploration. 
    \item Dropout probability : based on article \cite{srivastava_dropout_2014}, dropout is a method used to prevent over-fitting, by randomly fixing cells/layers to zeroes during one iteration. Being a probability, it's bounds by 0 and 1.
    \item Weight decay : weight decay is used to improve generalization capacity, as proved in article \cite{krogh_simple_1991}, by reducing the weights at each iterations by a small value, to force the model to use new inputs. Typically, the parameter for weight decay is set on a logarithmic scale between $10^{-3}$ and $10^{-1}$.
\end{itemize}





\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Hyper-parameter}} & \multicolumn{2}{|c|}{\textbf{Optimization range}} & \multirow{2}{*}{\textbf{Conversion}} \\
        \cline{2-3}
         & \textbf{Lower Bound} & \textbf{Upper Bound} &  \\
        \hline
        \textbf{Learning Rate} & $-10$ & $-1$ & $f(x) = 10^{x}$ \\
        \hline
        \textbf{LoRA Rank} & 2 & 32 & $f(x) = \text{round}(x)$ \\
        \hline
        \textbf{LoRA scale ($\alpha$)} & 16 & 64 & $f(x) = \text{round}(x)$ \\
        \hline
        \textbf{LoRA Dropout} & 0 & 0.5 & $f(x) = x$ \\
        \hline
        \textbf{Weight Decay} & $-3$ & $-1$ & $f(x) = 10^{x}$  \\
        \hline
    \end{tabular}
    \caption{Summary of Hyperparameter Search Space}
    \label{tab:hyperparam_table}
\end{table}

For the 2 integers variables (LoRA rank and LoRA scale), to adapt to continuous optimization algorithms, the relax and round methods will be applied. It mean that the integers constraints is relaxed when generating a solution, and is rounded when evaluating a solution. Others methods like computing with lower and upper discrete value can be used, but this one was kept for simplicity and computation costs.
For the 2 variables with logarithmic scale (learning rate and weight decay), to explore with consistency the search space, the optimization algorithm will be bound between the exponential values of the bounds, and a logarithm will be applied when evaluating a solution.

%%-------------------- SECTION : Optimization ---------------------%%
\section{Optimization}
\label{sec:opt}
To address the challenges of optimizing the black-box function, this research introduces two complementary approaches: Bayesian Optimization (BO) and a Partition-based method. BO is employed for its ability to efficiently navigate continuous hyperparameter spaces by balancing exploration and exploitation through surrogate modeling and acquisition functions. The Partition-based method divides the search space into regions, enabling parallel optimization and reducing redundancy in evaluations. By combining these methods, the framework achieves robust performance across diverse benchmarks. This integration is further enhanced by incorporating techniques such as multi-fidelity optimization and FlashAttention to improve computational efficiency and scalability.


%%-------------------- SECTION : Concrete implentation ---------------------%%
\section{Concrete Implementation}
\label{sec:concrete_impl}
Talk about OOP, which class I designed, with UML class diagramm

Final result (eval + training) : a class 

class diagramm ???

%%-------------------- SECTION : Experimental Setup ---------------------%%
\section{experiments setup}
\label{sec:exp_setup}

Grid5000, chuc etc.


%%-------------------- SECTION : Difficulties ---------------------%%
\section{Difficulties}
\label{sec:opt_difficulties}
Several challenges were encountered during the development and implementation of this optimization framework. The foremost difficulty is the high computational expense of evaluating LLMs, which necessitates the careful allocation of resources and the adoption of efficient evaluation strategies. Handling mixed-type hyperparameters, particularly the interplay between continuous and discrete variables, posed additional complexities. Existing optimization techniques often struggle with these mixed spaces, requiring innovative solutions such as relaxation and partitioning to ensure convergence. Finally, ensuring the generalizability of the fine-tuning results across different benchmarks and datasets proved challenging, as model performance is highly dependent on task-specific characteristics and dataset quality.