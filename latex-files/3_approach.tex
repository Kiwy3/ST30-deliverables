\chapter{Methodology}
\label{chap:methodo}
\epigraph{Everyone by now presumably knows about the danger of premature optimization. I think we should be just as worried about premature design - designing too early what a program should do.}{Paul Graham}

The methodology is a cornerstone of any research or project, providing a structured framework to achieve objectives systematically and effectively. It ensures clarity, reproducibility, and reliability by defining the steps, tools, and techniques used to address specific problems. A well-defined methodology not only aligns the research process with its goals but also facilitates critical evaluation by external audiences, allowing them to assess the validity and generalization of the results. In the context of this work, the chosen methodology was pivotal in navigating complex challenges, optimizing processes, and ensuring that outcomes are both credible and relevant.

To ensure my sincere approach, and contribute to open-source domain, all the code of this report is readable on  \href{https://github.com/Kiwy3/LLM_HPO_BO__st30}{my github account}\footnote{link : https://github.com/Kiwy3/LLM\_HPO\_BO\_\_st30}. It include every code snippet used in this report (like figure \ref{fig:soo_ex}), every experiments of chapter \ref{chap:results} and all implementation of \acrshort{hpo}.

In this chapter, I will talk about the contextualization in academic literature, then tackle the elaboration of the blackbox function. The definition of the search space being one of the most crucial step in global optimization, the section \ref{sec:search_space} will focus on this. After this preliminary work, we will enter the core of this report : optimization algorithms. A section about experimental setup, to explore resource and scientific integrity, will precede the conclusion section, approaching insight about the realization of this part.


\section{A Literature-Based Approach}
In industrial field of works, the goal is to be better than competitor, or at least be better than the past of the company. In research fields, a contribution must aims to be better than existing, at least by one facet. In order to do this, the first step of every research project is to make an exhaustive bibliography of the domain, to understand what's already done, and what could be the contribution of the project. 

Chapter \ref{chap:subject_def} was the result of a first stage of bibliography, to define what's the context of this internship. With this, we have insights and contexts about \acrshort{dnn},\acrshort{llm},\gls{fine_tuning} and \acrshort{peft}, and a first look at global optimization fields. In this chapter, a complementary approach will be done about specific optimization algorithm, frameworks and implementation specific details. 

At the beginning of this internship, I started my bibliography using few articles that my tutor send me, for a first look of the subject. From theses articles, I jumped to referenced articles until I started to make a loop between articles. It allow me to find fundational article like articles \cite{vaswani_attention_2017,talbi_automated_2021}, establishing the core of the domain, and reviews like article \cite{elsken_neural_2019,talbi_automated_2021}, allowing to understand a global context and finding a way to classify what I read before.

To manage my bibliography, in a first time I used Notion App\footnote{\href{https://www.notion.so}{https://www.notion.so}} to make a table for my bibliography, with papers charateristics (title, authors, year ...), an export of bibtex from original site and my notes. The table can be found on this \href{https://ribbon-crown-5f6.notion.site/6539799af4a24b32b6d4b91c4e07de49?v=b1542338391647aaa38cc8bb4ad1d5d8&pvs=4}{link}. When I started writing my article, I thought that it's wasn't pratical to copy bibtex export one by one, and I looked at others tools to manage this. It's how I found \href{https://www.zotero.org/}{Zotero}\footnote{link : \href{https://www.zotero.org/}{https://www.zotero.org/}}, with many options to ease my life like collecting article from web with only one click, and export a collection.

\section{Blackbox Elaboration}
\label{sec:blackbox}
My internship can be seen as global optimization applied to an expensive blackbox function. A blackbox function is a process that receive an input (here a set of hyperparameters), and return one (or multiple) value(s) (here the accuracy). The blackbox process here is described by figure \ref{fig:hpo_workflow}. This process start by the \gls{fine_tuning} of the model, using training dataset, and then evaluating the model, using the validation dataset. Next sections will explore in details the action box of figure \ref{fig:hpo_workflow}.


\begin{figure}
    \centering
    \input{assets/img/chap_3/hpo_workflow}
    \caption{HPO workflow}
    \label{fig:hpo_workflow}
\end{figure}

\subsection{Fine-Tuning of the Model}
Choose the model : TinyLlama and then LlaMa 3.2-3B
Choose the dataset

For the fine-tuning, I use \acrshort{lora} as defined in section \ref{sec:fine_tune}

Add lora figure

Fine tuning process is like general nn training, following algo X

Add algo of fine tuning

For the implementation : model from hf, class from litgpt, automation from lightning, with hooks

\subsection{Evaluation of the model}

To evaluate, two ways : loss or accuracy => why I choose acc, and even acc\_norm
=> fine tune model are zero shot learners

generic evaluation with algo X

I choose MMLU/HELLASWAG

Only one for HPO, but can eval on others to look at overfitting

implementation with lm\_eval

Final result (eval + training) : a class 

class diagramm ???

\section{Search Space Definition}
\label{search space}



\section{Optimization}
\label{sec:opt}
To address the challenges of optimizing the black-box function, this research introduces two complementary approaches: Bayesian Optimization (BO) and a Partition-based method. BO is employed for its ability to efficiently navigate continuous hyperparameter spaces by balancing exploration and exploitation through surrogate modeling and acquisition functions. The Partition-based method divides the search space into regions, enabling parallel optimization and reducing redundancy in evaluations. By combining these methods, the framework achieves robust performance across diverse benchmarks. This integration is further enhanced by incorporating techniques such as multi-fidelity optimization and FlashAttention to improve computational efficiency and scalability.

\section{experiments setup}
\label{sec:exp_setup}


\section{Difficulties}
Several challenges were encountered during the development and implementation of this optimization framework. The foremost difficulty is the high computational expense of evaluating LLMs, which necessitates the careful allocation of resources and the adoption of efficient evaluation strategies. Handling mixed-type hyperparameters, particularly the interplay between continuous and discrete variables, posed additional complexities. Existing optimization techniques often struggle with these mixed spaces, requiring innovative solutions such as relaxation and partitioning to ensure convergence. Finally, ensuring the generalizability of the fine-tuning results across different benchmarks and datasets proved challenging, as model performance is highly dependent on task-specific characteristics and dataset quality.