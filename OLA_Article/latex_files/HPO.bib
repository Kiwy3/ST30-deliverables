

@misc{tribes_hyperparameter_2024,
	title = {Hyperparameter {Optimization} for {Large} {Language} {Model} {Instruction}-{Tuning}},
	url = {http://arxiv.org/abs/2312.00949},
	doi = {10.48550/arXiv.2312.00949},
	abstract = {The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the {\textbackslash}nomad algorithm, achieving a boost in performance and human alignment of the tuned model.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Tribes, Christophe and Benarroch-Lelong, Sacha and Lu, Peng and Kobyzev, Ivan},
	month = jan,
	year = {2024},
	note = {arXiv:2312.00949},
	keywords = {Computer Science - Computation and Language, Fine-tuning, HPO, LLM, Mathematics - Optimization and Control},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/LCC9QEWN/Tribes et al. - 2024 - Hyperparameter Optimization for Large Language Model Instruction-Tuning.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/9R9QNLD4/2312.html:text/html},
}

@article{talbi_metaheuristics_2024,
	title = {Metaheuristics for variable-size mixed optimization problems: {A} unified taxonomy and survey},
	volume = {89},
	issn = {2210-6502},
	shorttitle = {Metaheuristics for variable-size mixed optimization problems},
	url = {https://www.sciencedirect.com/science/article/pii/S2210650224001809},
	doi = {10.1016/j.swevo.2024.101642},
	abstract = {Many real world optimization problems are formulated as mixed-variable optimization problems (MVOPs) which involve both continuous and discrete variables. MVOPs including dimensional variables are characterized by a variable-size search space. Depending on the values of dimensional variables, the number and type of the variables of the problem can vary dynamically. MVOPs and variable-size MVOPs (VMVOPs) are difficult to solve and raise a number of scientific challenges in the design of metaheuristics. Standard metaheuristics have been first designed to address continuous or discrete optimization problems, and are not able to tackle VMVOPs in an efficient way. The development of metaheuristics for solving such problems has attracted the attention of many researchers and is increasingly popular. However, to our knowledge there is no well established taxonomy or comprehensive survey for handling this important family of optimization problems. This paper presents an unified taxonomy for metaheuristic solutions for solving VMVOPs in an attempt to provide a common terminology and classification mechanisms. It provides a general mathematical formulation and concepts of VMVOPs, and identifies the various solving methodologies than can be applied in metaheuristics. The advantages, the weaknesses and the limitations of the presented methodologies are discussed. The proposed taxonomy also allows to identify some open research issues which needs further in-depth investigations.},
	urldate = {2024-11-20},
	journal = {Swarm and Evolutionary Computation},
	author = {Talbi, El-Ghazali},
	month = aug,
	year = {2024},
	keywords = {Decomposition-based optimization, Metaheuristics, Mixed optimization, Mixed variable programming, Mixed-variable optimization problem, Variable-size mixed-variable optimization problem, Variable-space design},
	pages = {101642},
	file = {ScienceDirect Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/BFLUB5AD/S2210650224001809.html:text/html;Submitted Version:/home/jan/snap/zotero-snap/common/Zotero/storage/HJKKM6H7/Talbi - 2024 - Metaheuristics for variable-size mixed optimization problems A unified taxonomy and survey.pdf:application/pdf},
}

@misc{LiuEmpirical2021,
	title = {An {Empirical} {Study} on {Hyperparameter} {Optimization} for {Fine}-{Tuning} {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2106.09204},
	doi = {10.48550/arXiv.2106.09204},
	abstract = {The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms' performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting. We propose two general strategies and an experimental procedure to systematically troubleshoot HPO's failure cases. By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains. Finally, we make suggestions for future work. Our implementation can be found in https://github.com/microsoft/FLAML/tree/main/flaml/nlp/.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Liu, Xueqing and Wang, Chi},
	month = jun,
	year = {2021},
	note = {arXiv:2106.09204 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear in ACL-IJCNLP 2021},
	file = {Preprint PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/Y8AM2S4M/Liu and Wang - 2021 - An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models.pdf:application/pdf;Snapshot:/home/jan/snap/zotero-snap/common/Zotero/storage/EVSYSUY5/2106.html:text/html},
}

@unpublished{firmin_fractal-based_2022,
	title = {A fractal-based decomposition framework for continuous optimization},
	url = {https://hal.science/hal-04474444},
	abstract = {In this paper, we propose a generic algorithmic framework which defines a unified view of fractal decomposition algorithms for continuous optimization. Fractals allow building a hierarchical decomposition of the decision space by using a self-similar geometrical object. The proposed generic framework is made of five distinct and independent search components: fractal geometrical object, tree search, scoring, exploration and exploitation. The genericity of the framework allowed the instantiation of popular algorithms from the optimization, machine learning and computational intelligence communities. Moreover, new optimization algorithms can be designed using various strategies of the search components. This shows the modularity of the proposed algorithmic framework. The computational experiments emphasize the behaviors of fractal-based approaches in terms of scalability, robustness, and the balance between exploitation and exploration in the search space. The obtained results show the significance of each search component of the fractal framework, and the necessity to build harder and well-defined benchmarks which can assess the performance of deterministic, axis-aligned and symmetrical decomposition-based algorithms.},
	urldate = {2025-01-07},
	author = {Firmin, Thomas and Talbi, El-Ghazali},
	month = jul,
	year = {2022},
	keywords = {Continuous optimization, Decomposition, Fractal, High-dimensional optimization, Metaheuristic, Tree search},
	annote = {working paper or preprint},
	file = {HAL PDF Full Text:C\:\\Users\\Nathan\\Zotero\\storage\\Y7SLEI8G\\Firmin et Talbi - 2022 - A fractal-based decomposition framework for continuous optimization.pdf:application/pdf},
}

@article{jones_lipschitzian_1993,
	title = {Lipschitzian optimization without the {Lipschitz} constant},
	volume = {79},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/BF00941892},
	doi = {10.1007/BF00941892},
	abstract = {We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods.},
	language = {en},
	number = {1},
	urldate = {2024-12-18},
	journal = {Journal of Optimization Theory and Applications},
	author = {Jones, D. R. and Perttunen, C. D. and Stuckman, B. E.},
	month = oct,
	year = {1993},
	keywords = {Global optimization, Lipschitzian optimization, space covering, space partitioning},
	pages = {157--181},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\TE6DU8EI\\Jones et al. - 1993 - Lipschitzian optimization without the Lipschitz constant.pdf:application/pdf},
}



@misc{gardner_gpytorch_2021,
	title = {{GPyTorch}: {Blackbox} {Matrix}-{Matrix} {Gaussian} {Process} {Inference} with {GPU} {Acceleration}},
	shorttitle = {{GPyTorch}},
	url = {http://arxiv.org/abs/1809.11165},
	doi = {10.48550/arXiv.1809.11165},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from \$O(n{\textasciicircum}3)\$ to \$O(n{\textasciicircum}2)\$. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = jun,
	year = {2021},
	note = {arXiv:1809.11165 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. Most recent version includes additional details on preconditioned BBMM},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\HZFEQGX7\\Gardner et al. - 2021 - GPyTorch Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration.pdf:application/pdf},
}

@misc{ament_unexpected_2024,
	title = {Unexpected {Improvements} to {Expected} {Improvement} for {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2310.20708},
	doi = {10.48550/arXiv.2310.20708},
	abstract = {Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Ament, Sebastian and Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan},
	month = jan,
	year = {2024},
	note = {arXiv:2310.20708 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2023 Spotlight},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\FILGLCA2\\Ament et al. - 2024 - Unexpected Improvements to Expected Improvement for Bayesian Optimization.pdf:application/pdf},
}

@inproceedings{wilson_efficiently_2020,
	title = {Efficiently sampling functions from {Gaussian} process posteriors},
	url = {https://proceedings.mlr.press/v119/wilson20a.html},
	abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10292--10302},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\2K2Q39ZC\\Wilson et al. - 2020 - Efficiently sampling functions from Gaussian process posteriors.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Nathan\\Zotero\\storage\\3YL7QH7A\\Wilson et al. - 2020 - Efficiently sampling functions from Gaussian process posteriors.pdf:application/pdf},
}

@article{borisut_adaptive_2023,
	title = {Adaptive {Latin} {Hypercube} {Sampling} for a {Surrogate}-{Based} {Optimization} with {Artificial} {Neural} {Network}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-9717},
	url = {https://www.mdpi.com/2227-9717/11/11/3232},
	doi = {10.3390/pr11113232},
	abstract = {A significant number of sample points are often required for surrogate-based optimization when utilizing process simulations to cover the entire system space. This necessity is particularly pronounced in complex simulations or high-dimensional physical experiments, where a large number of sample points is essential. In this study, we have developed an adaptive Latin hypercube sampling (LHS) method that generates additional sample points from areas with the highest output deviations to optimize the required number of samples. The surrogate model used for the optimization problem is artificial neural networks (ANNs). The standard for measuring solution accuracy is the percent error of the optimal solution. The outcomes of the proposed algorithm were compared to those of random sampling for validation. As case studies, we chose three different chemical processes to illustrate problems of varying complexity and numbers of variables. The findings indicate that for all case studies, the proposed LHS optimization algorithm required fewer sample points than random sampling to achieve optimal solutions of similar quality. To extend the application of this methodology, we recommend further applying it to fields beyond chemical engineering and higher-dimensional problems.},
	language = {en},
	number = {11},
	urldate = {2025-01-07},
	journal = {Processes},
	author = {Borisut, Prapatsorn and Nuchitprasittichai, Aroonsri},
	month = nov,
	year = {2023},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive Latin hypercube sampling, artificial neural network, design of experiment, process simulation, sequential sampling, simulation-based optimization},
	pages = {3232},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\LMLXAKET\\Borisut et Nuchitprasittichai - 2023 - Adaptive Latin Hypercube Sampling for a Surrogate-Based Optimization with Artificial Neural Network.pdf:application/pdf},
}

@article{rajaram_empirical_2021,
	title = {Empirical {Assessment} of {Deep} {Gaussian} {Process} {Surrogate} {Models} for {Engineering} {Problems}},
	volume = {58},
	issn = {0021-8669},
	url = {https://doi.org/10.2514/1.C036026},
	doi = {10.2514/1.C036026},
	abstract = {In recent years, multilayered hierarchical compositions of the well-known and widely used Gaussian process models called deep Gaussian processes are finding use in the approximation of black-box functions. In this paper, the performance of deep Gaussian process models is empirically evaluated and compared against the well-established Gaussian process models with a special emphasis on engineering problems. The work draws conclusions through detailed comparisons in terms of metrics such as computational training cost, data requirement, predictive error, and robustness to the choice of the initial design of experiments. Additionally, the viability and robustness of deep Gaussian process models for applications on practical engineering problems are analyzed through sensitivity to hyperparameters and scalability with respect to the input space dimensionality, respectively. Finally, the models are also compared in an adaptive construction setting, where they are built sequentially by selecting points that maximize posterior variance. Experiments are conducted on canonical test functions with varying input dimensions, an engineering test function, and a practical transonic airfoil test case with a high-dimensional input space. The experiments suggest that deep Gaussian process models outperform traditional Gaussian process models in terms of accuracy at the cost of incurring a significantly higher computational expense for the training procedure. The sensitivity studies indicate that inducing points is the most important hyperparameter that affects deep Gaussian process performance and training time. This work empirically shows that deep Gaussian processes are promising candidates for problems that are known to be nonlinear and high-dimensional, and when limited training data are available.},
	number = {1},
	urldate = {2025-01-07},
	journal = {Journal of Aircraft},
	author = {Rajaram, Dushhyanth and Puranik, Tejas G. and Ashwin Renganathan, S. and Sung, WoongJe and Fischer, Olivia Pinon and Mavris, Dimitri N. and Ramamurthy, Arun},
	year = {2021},
	note = {Publisher: American Institute of Aeronautics and Astronautics
\_eprint: https://doi.org/10.2514/1.C036026},
	pages = {182--196},
}

@article{ammari_linear_2023,
	title = {Linear model decision trees as surrogates in optimization of engineering applications},
	volume = {178},
	issn = {0098-1354},
	url = {https://www.sciencedirect.com/science/article/pii/S009813542300217X},
	doi = {10.1016/j.compchemeng.2023.108347},
	abstract = {Machine learning models are promising as surrogates in optimization when replacing difficult to solve equations or black-box type models. This work demonstrates the viability of linear model decision trees as piecewise-linear surrogates in decision-making problems. Linear model decision trees can be represented exactly in mixed-integer linear programming (MILP) and mixed-integer quadratic constrained programming (MIQCP) formulations. Furthermore, they can represent discontinuous functions, bringing advantages over neural networks in some cases. We present several formulations using transformations from Generalized Disjunctive Programming (GDP) formulations and modifications of MILP formulations for gradient boosted decision trees (GBDT). We then compare the computational performance of these different MILP and MIQCP representations in an optimization problem and illustrate their use on engineering applications. We observe faster solution times for optimization problems with linear model decision tree surrogates when compared with GBDT surrogates using the Optimization and Machine Learning Toolkit (OMLT).},
	urldate = {2025-01-07},
	journal = {Computers \& Chemical Engineering},
	author = {Ammari, Bashar L. and Johnson, Emma S. and Stinchfield, Georgia and Kim, Taehun and Bynum, Michael and Hart, William E. and Pulsipher, Joshua and Laird, Carl D.},
	month = oct,
	year = {2023},
	keywords = {Decision trees, Machine learning, Optimization, Piecewise-linear surrogates},
	pages = {108347},
}


@article{zhou_lima_2023,
	title = {{LIMA}: {Less} {Is} {More} for {Alignment}},
	volume = {36},
	shorttitle = {{LIMA}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-07},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
	month = dec,
	year = {2023},
	pages = {55006--55021},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\RRIYY2D3\\Zhou et al. - 2023 - LIMA Less Is More for Alignment.pdf:application/pdf},
}

@article{chung_scaling_2024,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	volume = {25},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v25/23-0870.html},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks (at time of release), such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	number = {70},
	urldate = {2025-01-07},
	journal = {Journal of Machine Learning Research},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	year = {2024},
	pages = {1--53},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\A2LIDV8E\\Chung et al. - 2024 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf},
}

@article{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	volume = {36},
	shorttitle = {{QLoRA}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html},
	language = {en},
	urldate = {2024-12-13},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = dec,
	year = {2023},
	pages = {10088--10115},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\W8DVRC5S\\Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}


@misc{xu_gps_2022,
	title = {{GPS}: {Genetic} {Prompt} {Search} for {Efficient} {Few}-shot {Learning}},
	shorttitle = {{GPS}},
	url = {http://arxiv.org/abs/2210.17041},
	doi = {10.48550/arXiv.2210.17041},
	abstract = {Prompt-based techniques have demostrated great potential for improving the few-shot generalization of pretrained language models. However, their performance heavily relies on the manual design of prompts and thus requires a lot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS) to improve few-shot learning with prompts, which utilizes a genetic algorithm to automatically search for high-performing prompts. GPS is gradient-free and requires no update of model parameters but only a small validation set. Experiments on diverse datasets proved the effectiveness of GPS, which outperforms manual prompts by a large margin of 2.6 points. Our method is also better than other parameter-efficient tuning methods such as prompt tuning.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
	month = oct,
	year = {2022},
	note = {arXiv:2210.17041 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 10 pages},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\FLQ9WGYL\\Xu et al. - 2022 - GPS Genetic Prompt Search for Efficient Few-shot Learning.pdf:application/pdf},
}


@misc{diao_black-box_2023,
	title = {Black-box {Prompt} {Learning} for {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2201.08531},
	doi = {10.48550/arXiv.2201.08531},
	abstract = {The increasing scale of general-purpose Pre-trained Language Models (PLMs) necessitates the study of more efficient adaptation across different downstream tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL) to resonate with pragmatic interactions between the cloud infrastructure and edge devices. Particularly, instead of fine-tuning the model in the cloud, we adapt PLMs by prompt learning, which efficiently optimizes only a few parameters of the discrete prompts. Moreover, we consider the scenario that we do not have access to the parameters and gradients of the pre-trained models, except for its outputs given inputs. This black-box setting secures the cloud infrastructure from potential attack and misuse to cause a single-point failure, which is preferable to the white-box counterpart by current infrastructures. Under this black-box constraint, we apply a variance-reduced policy gradient algorithm to estimate the gradients of parameters in the categorical distribution of each discrete prompt. In light of our method, the user devices can efficiently tune their tasks by querying the PLMs bounded by a range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the proposed algorithm achieves significant improvement on eight benchmarks in a cloud-device collaboration manner. Finally, we conduct in-depth case studies to comprehensively analyze our method in terms of various data sizes, prompt lengths, training budgets, optimization objectives, prompt transferability, and explanations of the learned prompts. Our code will be available at https://github.com/shizhediao/Black-Box-Prompt-Learning.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Diao, Shizhe and Huang, Zhichao and Xu, Ruijia and Li, Xuechun and Lin, Yong and Zhou, Xiao and Zhang, Tong},
	month = feb,
	year = {2023},
	note = {arXiv:2201.08531 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear in the Transactions on Machine Learning Research (TMLR)},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\ZHZTGK9Z\\Diao et al. - 2023 - Black-box Prompt Learning for Pre-trained Language Models.pdf:application/pdf},
}


@misc{brahmachary_large_2024,
	title = {Large {Language} {Model}-{Based} {Evolutionary} {Optimizer}: {Reasoning} with elitism},
	shorttitle = {Large {Language} {Model}-{Based} {Evolutionary} {Optimizer}},
	url = {http://arxiv.org/abs/2403.02054},
	doi = {10.48550/arXiv.2403.02054},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Brahmachary, Shuvayan and Joshi, Subodh M. and Panda, Aniruddha and Koneripalli, Kaushik and Sagotra, Arun Kumar and Patel, Harshil and Sharma, Ankush and Jagtap, Ameya D. and Kalyanaraman, Kaushic},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02054 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\MTYBLUPU\\Brahmachary et al. - 2024 - Large Language Model-Based Evolutionary Optimizer Reasoning with elitism.pdf:application/pdf},
}


@inproceedings{cai_exploring_2024,
	address = {New York, NY, USA},
	series = {{GECCO} '24 {Companion}},
	title = {Exploring the {Improvement} of {Evolutionary} {Computation} via {Large} {Language} {Models}},
	isbn = {979-8-4007-0495-6},
	url = {https://dl.acm.org/doi/10.1145/3638530.3664086},
	doi = {10.1145/3638530.3664086},
	abstract = {Evolutionary computation (EC), as a powerful optimization algorithm, has been applied across various domains. However, as the complexity of problems increases, the limitations of EC have become more apparent. The advent of large language models (LLMs) has not only transformed natural language processing but also extended their capabilities to diverse fields. By harnessing LLMs' vast knowledge and adaptive capabilities, we provide a forward-looking overview of potential improvements LLMs can bring to EC, focusing on the algorithms themselves, population design, and additional enhancements. This presents a promising direction for future research at the intersection of LLMs and EC.},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {Association for Computing Machinery},
	author = {Cai, Jinyu and Xu, Jinglue and Li, Jialong and Yamauchi, Takuto and Iba, Hitoshi and Tei, Kenji},
	month = aug,
	year = {2024},
	pages = {83--84},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\7V8ZS3CS\\Cai et al. - 2024 - Exploring the Improvement of Evolutionary Computation via Large Language Models.pdf:application/pdf},
}


@inproceedings{liu_large_2024,
	title = {Large {Language} {Models} as {Evolutionary} {Optimizers}},
	url = {https://ieeexplore.ieee.org/abstract/document/10611913},
	doi = {10.1109/CEC60901.2024.10611913},
	abstract = {Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self- adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.},
	urldate = {2025-01-07},
	booktitle = {2024 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	author = {Liu, Shengcai and Chen, Caishun and Qu, Xinghua and Tang, Ke and Ong, Yew-Soon},
	month = jun,
	year = {2024},
	keywords = {EA, LLM, Training, combinatorial optimization, Computational modeling, Evolutionary algorithms, Evolutionary computation, large language model, Large language models, pre-trained model, Sociology, Temperature control, Traveling salesman problems, LLMtoEA},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\UBKFSRMA\\Liu et al. - 2024 - Large Language Models as Evolutionary Optimizers.pdf:application/pdf},
}


@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{hendrycks2021ethics,
  title={Aligning AI With Shared Human Values},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andrew Critch and Jerry Li and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}


@misc{wei_finetuned_2022,
	title = {Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2109.01652},
	doi = {10.48550/arXiv.2109.01652},
	abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
	month = feb,
	year = {2022},
	note = {arXiv:2109.01652 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Version 5. Find list of changes in Appendix F (page 35)},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\XA5AX2KI\\Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\B75H7GUF\\Loshchilov et Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf},
}

@inproceedings{krogh_simple_1991,
	title = {A {Simple} {Weight} {Decay} {Can} {Improve} {Generalization}},
	volume = {4},
	url = {https://papers.nips.cc/paper_files/paper/1991/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html},
	abstract = {It has  been observed  in  numerical simulations that a weight decay  can  im(cid:173) prove generalization in a feed-forward  neural network.  This paper explains  why.  It is  proven  that  a  weight  decay  has  two effects  in  a  linear  network.  First,  it  suppresses  any  irrelevant  components  of  the  weight  vector  by  choosing  the smallest  vector  that solves  the learning  problem.  Second,  if  the size  is chosen  right,  a weight  decay  can suppress some of the effects  of  static  noise  on  the  targets,  which  improves  generalization  quite  a  lot.  It  is  then  shown  how  to extend  these  results  to networks  with hidden  layers  and  non-linear  units.  Finally  the  theory  is  confirmed  by  some numerical  simulations using  the  data from  NetTalk.},
	urldate = {2025-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Krogh, Anders and Hertz, John},
	year = {1991},
}

@phdthesis{balouek_adding_2012,
	type = {report},
	title = {Adding {Virtualization} {Capabilities} to {Grid}'5000},
	url = {https://inria.hal.science/hal-00720910},
	abstract = {Almost ten years after its premises, the Grid'5000 testbed has become one of the most complete testbed for designing or evaluating large-scale distributed systems. Initially dedicated to the study of High Performance Computing, the infrastructure has evolved to address wider concerns related to Desktop Computing, the Internet of Services and more recently the Cloud Computing paradigm. This report present recent improvements of the Grid'5000 software and services stack to support large-scale experiments using virtualization technologies as building blocks. Such contributions include the deployment of customized software environments, the reservation of dedicated network domain and the possibility to isolate them from the others, and the automation of experiments with a REST API. We illustrate the interest of these contributions by describing three different use-cases of large-scale experiments on the Grid'5000 testbed. The first one leverages virtual machines to conduct larger experiments spread over 4000 peers. The second one describes the deployment of 10000 KVM instances over 4 Grid'5000 sites. Finally, the last use case introduces a one-click deployment tool to easily deploy major IaaS solutions. The conclusion highlights some important challenges of Grid'5000 related to the use of OpenFlow and to the management of applications dealing with tremendous amount of data.},
	language = {en},
	urldate = {2025-01-06},
	school = {INRIA},
	author = {Balouek, Daniel and Carpen-Amarie, Alexandra and Charrier, Ghislain and Desprez, Frédéric and Jeannot, Emmanuel and Jeanvoine, Emmanuel and Lebre, Adrien and Margery, David and Niclausse, Nicolas and Nussbaum, Lucas and Richard, Olivier and Pérez, Christian and Quesnel, Flavien and Rohr, Cyril and Sarzyniec, Luc},
	month = jul,
	year = {2012},
	doi = {10/document},
	note = {Pages: 18},
}


@misc{balandat_botorch_2020,
	title = {{BoTorch}: {A} {Framework} for {Efficient} {Monte}-{Carlo} {Bayesian} {Optimization}},
	shorttitle = {{BoTorch}},
	url = {http://arxiv.org/abs/1910.06403},
	doi = {10.48550/arXiv.1910.06403},
	abstract = {Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel "one-shot" formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
	month = dec,
	year = {2020},
	note = {arXiv:1910.06403 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\CW3GRD6K\\Balandat et al. - 2020 - BoTorch A Framework for Efficient Monte-Carlo Bayesian Optimization.pdf:application/pdf},
}


@misc{ansel_pytorch_2024,
	title = {{PyTorch} 2: {Faster} {Machine} {Learning} {Through} {Dynamic} {Python} {Bytecode} {Transformation} and {Graph} {Compilation}},
	shorttitle = {{PyTorch} 2},
	url = {https://pytorch.org/assets/pytorch2-2.pdf},
	abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration},
	urldate = {2025-01-06},
	publisher = {ACM},
	author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
	month = apr,
	year = {2024},
	doi = {10.1145/3620665.3640366},
	note = {Publication Title: 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)
original-date: 2016-08-13T05:26:41Z},
}

@misc{the_lightning_ai_team_litgpt_2023,
	title = {{LitGPT}},
	copyright = {Apache-2.0},
	url = {https://github.com/Lightning-AI/litgpt},
	abstract = {20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.},
	urldate = {2025-01-06},
	author = {{The Lightning AI team}},
	month = mar,
	year = {2023},
	note = {original-date: 2023-05-04T17:46:11Z},
}

@misc{zellers_hellaswag_2019,
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {http://arxiv.org/abs/1905.07830},
	doi = {10.48550/arXiv.1905.07830},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	month = may,
	year = {2019},
	note = {arXiv:1905.07830 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2019. Project page at https://rowanzellers.com/hellaswag},
}

@article{nakib_deterministic_2017,
	title = {Deterministic metaheuristic based on fractal decomposition for large-scale optimization},
	volume = {61},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494617304623},
	doi = {10.1016/j.asoc.2017.07.042},
	abstract = {In this work a new method based on geometric fractal decomposition to solve large-scale continuous optimization problems is proposed. It consists of dividing the feasible search space into sub-regions with the same geometrical pattern. At each iteration, the most promising ones are selected and further decomposed. This approach tends to provide a dense set of samples and has interesting theoretical convergence properties. Under some assumptions, this approach covers all the search space only in case of small dimensionality problems. The aim of this work is to propose a new algorithm based on this approach with low complexity and which performs well in case of large-scale problems. To do so, a low complex method that profits from fractals properties is proposed. Then, a deterministic optimization procedure is proposed using a single solution-based metaheuristic which is exposed to illustrate the performance of this strategy. Obtained results on common test functions were compared to those of algorithms from the literature and proved the efficiency of the proposed algorithm.},
	urldate = {2024-11-20},
	journal = {Applied Soft Computing},
	author = {Nakib, A. and Ouchraa, S. and Shvai, N. and Souquet, L. and Talbi, E. -G.},
	month = dec,
	year = {2017},
	keywords = {FDA, Geometric fractal decomposition, Large-scale optimization, Local search continuous optimization, Metaheuristics, PBO},
	pages = {468--485},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\PDY5IXJZ\\Nakib et al. - 2017 - Deterministic metaheuristic based on fractal decomposition for large-scale optimization.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\H9SW3PX6\\S1568494617304623.html:text/html},
}

@article{shahriari_taking_2016,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
	volume = {104},
	issn = {1558-2256},
	shorttitle = {Taking the {Human} {Out} of the {Loop}},
	url = {https://ieeexplore.ieee.org/abstract/document/7352306/authors#authors},
	doi = {10.1109/JPROC.2015.2494218},
	abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
	number = {1},
	urldate = {2024-12-13},
	journal = {Proceedings of the IEEE},
	author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
	month = jan,
	year = {2016},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Bayes methods, Big data, decision making, Decision making, design of experiments, Design of experiments, Genomes, genomic medicine, Linear programming, optimization, Optimization, response surface methodology, Statistical analysis, statistical learning},
	pages = {148--175},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\NZSM4Q4R\\Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:application/pdf},
}


@misc{valipour_dylora_2023,
	title = {{DyLoRA}: {Parameter} {Efficient} {Tuning} of {Pre}-trained {Models} using {Dynamic} {Search}-{Free} {Low}-{Rank} {Adaptation}},
	shorttitle = {{DyLoRA}},
	url = {http://arxiv.org/abs/2210.07558},
	doi = {10.48550/arXiv.2210.07558},
	abstract = {With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry. As a remedy, low-rank adapters (LoRA) keep the main pretrained weights of the model frozen and just introduce some learnable truncated SVD modules (so-called LoRA blocks) to the model. While LoRA blocks are parameter-efficient, they suffer from two major problems: first, the size of these blocks is fixed and cannot be modified after training (for example, if we need to change the rank of LoRA blocks, then we need to re-train them from scratch); second, optimizing their rank requires an exhaustive search and effort. In this work, we introduce a dynamic low-rank adaptation (DyLoRA) technique to address these two problems together. Our DyLoRA method trains LoRA blocks for a range of ranks instead of a single rank by sorting the representation learned by the adapter module at different ranks during training. We evaluate our solution on different natural language understanding (GLUE benchmark) and language generation tasks (E2E, DART and WebNLG) using different pretrained models such as RoBERTa and GPT with different sizes. Our results show that we can train dynamic search-free models with DyLoRA at least 4 to 7 times (depending to the task) faster than LoRA without significantly compromising performance. Moreover, our models can perform consistently well on a much larger range of ranks compared to LoRA.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Valipour, Mojtaba and Rezagholizadeh, Mehdi and Kobyzev, Ivan and Ghodsi, Ali},
	month = apr,
	year = {2023},
	note = {arXiv:2210.07558 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to EACL 2023},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\NWEYYHUB\\Valipour et al. - 2023 - DyLoRA Parameter Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptati.pdf:application/pdf},
}

@misc{han_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	month = sep,
	year = {2024},
	note = {arXiv:2403.14608 [cs]},
	keywords = {Computer Science - Machine Learning, PEFT},
	annote = {Comment: 25 pages, 12 figures. Due to word limit, the abstract here is truncated. The full abstract is available in the PDF},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\FBITC7M4\\Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey.pdf:application/pdf},
}

@incollection{feurer_hyperparameter_2019,
	address = {Cham},
	title = {Hyperparameter {Optimization}},
	isbn = {978-3-030-05318-5},
	url = {https://doi.org/10.1007/978-3-030-05318-5_1},
	abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
	language = {en},
	urldate = {2024-12-13},
	booktitle = {Automated {Machine} {Learning}: {Methods}, {Systems}, {Challenges}},
	publisher = {Springer International Publishing},
	author = {Feurer, Matthias and Hutter, Frank},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	year = {2019},
	doi = {10.1007/978-3-030-05318-5_1},
	pages = {3--33},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\9DDQHHXG\\Feurer et Hutter - 2019 - Hyperparameter Optimization.pdf:application/pdf},
}

@inproceedings{bergstra_algorithms_2011,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	volume = {24},
	url = {https://inria.hal.science/hal-00642998},
	abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap- proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos- sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu- ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex- pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli- able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	language = {en},
	urldate = {2024-11-20},
	publisher = {Neural Information Processing Systems Foundation},
	author = {Bergstra, James and Bardenet, R. and Bengio, Yoshua and Kégl, Balázs},
	month = dec,
	year = {2011},
	keywords = {HPO, Review},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\WZXI4XMK\\Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf:application/pdf},
}


@misc{bischl_hyperparameter_2021,
	title = {Hyperparameter {Optimization}: {Foundations}, {Algorithms}, {Best} {Practices} and {Open} {Challenges}},
	shorttitle = {Hyperparameter {Optimization}},
	url = {http://arxiv.org/abs/2107.05847},
	doi = {10.48550/arXiv.2107.05847},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	month = nov,
	year = {2021},
	note = {arXiv:2107.05847 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\7C239S9Q\\Bischl et al. - 2021 - Hyperparameter Optimization Foundations, Algorithms, Best Practices and Open Challenges.pdf:application/pdf},
}




@misc{hayou_lora_2024,
	title = {{LoRA}+: {Efficient} {Low} {Rank} {Adaptation} of {Large} {Models}},
	shorttitle = {{LoRA}+},
	url = {http://arxiv.org/abs/2402.12354},
	doi = {10.48550/arXiv.2402.12354},
	abstract = {In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA\$+\$. In our extensive experiments, LoRA\$+\$ improves performance (1-2 \${\textbackslash}\%\$ improvements) and finetuning speed (up to \${\textbackslash}sim\$ 2X SpeedUp), at the same computational cost as LoRA.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
	month = jul,
	year = {2024},
	note = {arXiv:2402.12354 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 27 pages},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\EXFGCJ5H\\Hayou et al. - 2024 - LoRA+ Efficient Low Rank Adaptation of Large Models.pdf:application/pdf},
}

@article{talbi_automated_2021,
	title = {Automated {Design} of {Deep} {Neural} {Networks}: {A} {Survey} and {Unified} {Taxonomy}},
	volume = {54},
	issn = {0360-0300},
	shorttitle = {Automated {Design} of {Deep} {Neural} {Networks}},
	url = {https://dl.acm.org/doi/10.1145/3439730},
	doi = {10.1145/3439730},
	abstract = {In recent years, research in applying optimization approaches in the automatic design of deep neural networks has become increasingly popular. Although various approaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this hot research topic. In this article, we propose a unified way to describe the various optimization algorithms that focus on common and important search components of optimization algorithms: representation, objective function, constraints, initial solution(s), and variation operators. In addition to large-scale search space, the problem is characterized by its variable mixed design space, it is very expensive, and it has multiple blackbox objective functions. Hence, this unified methodology has been extended to advanced optimization approaches, such as surrogate-based, multi-objective, and parallel optimization.},
	number = {2},
	urldate = {2024-11-20},
	journal = {ACM Comput. Surv.},
	author = {Talbi, El-Ghazali},
	month = mar,
	year = {2021},
	keywords = {autoDNN, DNN, Survey},
	pages = {34:1--34:37},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\4HF649T2\\Talbi - 2021 - Automated Design of Deep Neural Networks A Survey and Unified Taxonomy.pdf:application/pdf},
}

@inproceedings{munos_optimistic_2011,
	title = {Optimistic {Optimization} of a {Deterministic} {Function} without the {Knowledge} of its {Smoothness}},
	volume = {24},
	url = {https://proceedings.neurips.cc/paper/2011/hash/7e889fb76e0e07c11733550f2a6c7a5a-Abstract.html},
	abstract = {We consider a global optimization problem of a deterministic function f in a semimetric space, given a finite budget of n evaluations. The function f is assumed to be locally smooth (around one of its global maxima) with respect to a semi-metric . We describe two algorithms based on optimistic exploration that use a hierarchical partitioning of the space at all scales. A first contribution is an algorithm, DOO, that requires the knowledge of . We report a finite-sample performance bound in terms of a measure of the quantity of near-optimal states. We then define a second algorithm, SOO, which does not require the knowledge of the semimetric  under which f is smooth, and whose performance is almost as good as DOO optimally-fitted.},
	urldate = {2025-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Munos, Rémi},
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\JNGWVNTJ\\Munos - 2011 - Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness.pdf:application/pdf},
}


@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\ATD4HEXB\\Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\PEDE2NBR\\2106.html:text/html},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-11-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Nathan\\Zotero\\storage\\BIG93IT6\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@misc{wang_bayesian_2014,
	title = {Bayesian {Multi}-{Scale} {Optimistic} {Optimization}},
	url = {http://arxiv.org/abs/1402.7005},
	abstract = {Bayesian optimization is a powerful global optimization technique for expensive black-box functions. One of its shortcomings is that it requires auxiliary optimization of an acquisition function at each iteration. This auxiliary optimization can be costly and very hard to carry out in practice. Moreover, it creates serious theoretical concerns, as most of the convergence results assume that the exact optimum of the acquisition function can be found. In this paper, we introduce a new technique for efficient global optimization that combines Gaussian process confidence bounds and treed simultaneous optimistic optimization to eliminate the need for auxiliary optimization of acquisition functions. The experiments with global optimization benchmarks and a novel application to automatic information extraction demonstrate that the resulting technique is more efficient than the two approaches from which it draws inspiration. Unlike most theoretical analyses of Bayesian optimization with Gaussian processes, our finite-time convergence rate proofs do not require exact optimization of an acquisition function. That is, our approach eliminates the unsatisfactory assumption that a difficult, potentially NP-hard, problem has to be solved in order to obtain vanishing regret rates.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Wang, Ziyu and Shakibi, Babak and Jin, Lin and Freitas, Nando de},
	month = feb,
	year = {2014},
	note = {arXiv:1402.7005 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, BaMSOO},
	annote = {Comment: 15 pages Funding article of BaMSOO
    },
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\HYMD2UAY\\Wang et al. - 2014 - Bayesian Multi-Scale Optimistic Optimization.pdf:application/pdf;Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\TZ6K6S44\\1402.html:text/html},
}

@misc{narayanan_efficient_2021,
	title = {Efficient {Large}-{Scale} {Language} {Model} {Training} on {GPU} {Clusters} {Using} {Megatron}-{LM}},
	url = {http://arxiv.org/abs/2104.04473},
	doi = {10.48550/arXiv.2104.04473},
	abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
	month = aug,
	year = {2021},
	note = {arXiv:2104.04473},
	keywords = {Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\PY6MIH7V\\Narayanan et al. - 2021 - Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.pdf:application/pdf;Snapshot:C\:\\Users\\Nathan\\Zotero\\storage\\84PUEWU2\\2104.html:text/html},
}


@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	file = {PDF:/home/jan/snap/zotero-snap/common/Zotero/storage/GU4LHRUE/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks from Overﬁtting.pdf:application/pdf},
}


@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\AGELEQGK\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-12-13},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Wyatt, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Guzmán, Francisco and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Thattai, Govind and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Zhang, Jack and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and Linde, Jelmer van der and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Prasad, Karthik and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Lakhotia, Kushal and Rantala-Yeary, Lauren and Maaten, Laurens van der and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and Oliveira, Luke de and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Tsimpoukelli, Maria and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Zhang, Ning and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Maheswari, Rohan and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Albiero, Vítor and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Wang, Xiaofang and Tan, Xiaoqing Ellen and Xia, Xide and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Srivastava, Aayushi and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Teo, Amos and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Dong, Annie and Franco, Annie and Goyal, Anuj and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and Paola, Beto De and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Liu, Ce and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Gao, Cynthia and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Le, Eric-Tuan and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Kokkinos, Filippos and Ozgenel, Firat and Caggioni, Francesco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Inan, Hakan and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Zhan, Hongyuan and Damlaj, Ibrahim and Molybog, Igor and Tufanov, Igor and Leontiadis, Ilias and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Lam, Janice and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Jagadeesh, Kiran and Huang, Kun and Chawla, Kunal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Liu, Miao and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Mehta, Nikhil and Laptev, Nikolay Pavlovich and Dong, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Parthasarathy, Rangaprabhu and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Howes, Russ and Rinott, Ruty and Mehta, Sachin and Siby, Sachin and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Mahajan, Saurabh and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Patil, Shishir and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Deng, Summer and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Koehler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wu, Xiaojian and Wang, Xiaolan and Wu, Xilun and Gao, Xinbo and Kleinman, Yaniv and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Zhao, Yu and Hao, Yuchen and Qian, Yundi and Li, Yunlu and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei and Ma, Zhiyu},
	month = nov,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Nathan\\Zotero\\storage\\ASF9A62K\\Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf},
}
