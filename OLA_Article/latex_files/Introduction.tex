\section{Introduction}
\label{sec:intro}

With the advent of transformer architecture\cite{vaswani_attention_2017}, coupled with significant advancements in the field of \acrfull{hpc}, \acrfull{llm}s have demonstrated exceptional abilities in language understanding and text generation, firmly establishing themselves as the \acrfull{sota} in \acrfull{nlp}. Models such as GPT-4 \cite{openai_gpt-4_2024} and LLaMA 3 \cite{grattafiori_llama_2024} have rapidly penetrated the personal and professional domains, transforming workflows in academic, industrial and everyday applications.

Despite their success, the computational and data requirements for training LLMs from scratch make this approach impractical for most users. Fine-tuning has emerged as a critical paradigm, adapting pre-trained models to specialized tasks using smaller domain-specific datasets. Although fine-tuning reduces costs compared to full training, it remains resource intensive, particularly for large-scale models. To address this, \acrfull{peft} methods \cite{han_parameter-efficient_2024}, such as Low-Rank Adaptation (LoRA) \cite{hu_lora_2021}, have become prevalent. LoRA and its derivatives (e.g., QLoRA \cite{dettmers_qlora_2023}, LoRA + \cite{hayou_lora_2024}) reduce computational overhead by using reparametrized weight matrices, where large weight matrices are approximated with lower-rank decomposition. This approach enables fine-tuning even on limited-hardware setups.

Even with advancements in \acrshort{peft}, a significant barrier persists: selecting optimal hyperparameters. Unlike model parameters, which are learned during training, hyperparameters must be manually specified and profoundly influence model performance. For \acrshort{lora}-based fine-tuning, hyperparameters such as rank and scaling factors are critical\cite{valipour_dylora_2023}, along with classical parameters such as learning rate and weight decay.

\acrfull{hpo} is a well-studied problem \cite{bischl_hyperparameter_2021,bergstra_algorithms_2011,feurer_hyperparameter_2019} in \acrfull{autodnn} design \cite{talbi_automated_2021}. However, \acrshort{hpo} for LLM fine-tuning presents unique challenges due to the prohibitive cost of evaluating the objective function. Each evaluation entails computationally expensive training or fine-tuning process, often requiring hours to days on high-performance hardware. These constraints necessitate the adoption of highly efficient optimization techniques.

\acrfull{bo} \cite{shahriari_taking_2016} is a widely recognized approach for tackling expensive black-box optimization problems. Its ability to model the search space using a surrogate function allows it to minimize costly evaluations. However, \acrshort{bo} is by nature sequential, limiting the use of \acrshort{hpc} cluster, or exascale computer.
In contrast, \acrfull{pbo} methods \cite{nakib_deterministic_2017,munos_optimistic_2011}, which divide the search space into subregions, enable parallel evaluations but typically require more function calls to converge. The complementarity of these methods made way for hybrid algorithms, using surrogate to enhance \acrshort{pbo} algorithms.

In this work, we present the \acrshort{hpo} problem for \acrshort{llm} fine-tuning, to address it with \acrlong{bo} and \acrlong{pbo} methods. By comparing these methods, and trying the hybridization, we aim to open the way for parallel Bayesian algorithm for optimization of very expensive objective function with exascale computing.
