
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LLM %%%%%%%%%%%%%%%%%%
\begin{frame}{Large Language Models}
   \begin{columns}
         
    %%%%%%%%%%%%%%%%%%%%%%%%%% COLONNE DE GAUCHE %%%%%%%%%%%%%%
       \begin{column}[t]{0.45\textwidth} 
       \begin{block}{Point clés}
           \begin{itemize}
               \item Etat de l'art pour le traitement de language naturel.
               \item Réseaux de Neurones avec une architecture basé sur le  transformer\footnote{Vaswani et al, Attention is all you need,2017 } (annexe \ref{ap:llm_architecture})
               \item Taille : entre 1 et 405 Milliards de neurones
           \end{itemize}
               
       \end{block}
       \end{column}
           
    %%%%%%%%%%%%%%%%%%%%%%%%% COLONNE DE DROITE %%%%%%%%%%%%%%
       \begin{column}[t]{0.5\textwidth}
       \begin{block}{Auto-attention}
           \begin{figure}
               \centering
               \input{assets/tikz_picture/self_attention.tex}
               \caption{Illustration du mécanisme d'auto-attention}
           \end{figure}    
           L'auto-attention est la clé du LLM, en permettant de comprendre le contexte
       \end{block}  
       \end{column}
            
   \end{columns}
   \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fine Tuning %%%%%%%%%%%%%%%%%%
\begin{frame}{Fine Tuning}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            \textbf{Aspect} & \textbf{Pre-entrainement} & \textbf{Fine Tuning} \\
            \hline
            Objectif & Apprentissage general & Adaptation à un domaine \\
            \hline
            Données & Larges et diverses & Restreintes et Spécifiques \\
            \hline
            Ressources & Centaines de GPU & au moins 1 GPU \\
            \hline
            Durée & Semaine/Mois & Heures/Jours \\
            \hline
        \end{tabular}
        \caption{Comparaison entre le Pre-entrainement et le Fine Tuning de LLM}
        \label{tab:pretrain_vs_finetune}
    \end{table}

    \begin{block}{Parameter-Efficient Fine-Tuning (PEFT)}
        \begin{itemize}
            \item Ensemble de méthodes pour réduire le nombre de paramètres à entrainer
            \item Utilisation de la méthode LoRA (annexe \ref{ap:lora})
            \item Amène des nouveaux hyperparamètres
        \end{itemize}
        
    \end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% HPO %%%%%%%%%%%%%%%%%%
\begin{frame}{Optimisation des Hyperparamètres (OHP)}
   \begin{columns}
         
    %%%%%%%%%%%%%%%%%%%%%%%%%% COLONNE DE GAUCHE %%%%%%%%%%%%%%
       \begin{column}[t]{0.3\textwidth} 
       \begin{block}{Hyperparamètres}
         Paramètres qui ne sont pas entrainés par le modèle (learning rate, dropout ...)           
       \end{block}
       \begin{block}{Objectifs}
        \begin{itemize}
            \item Meilleur performance qu'en manuel
            \item Retirer le besoin d'expertise
        \end{itemize}
        
       \end{block}

       \end{column}
           
    %%%%%%%%%%%%%%%%%%%%%%%%% COLONNE DE DROITE %%%%%%%%%%%%%%
       \begin{column}[t]{0.8\textwidth}
        \begin{figure}
            \centering
            \input{assets/tikz_picture/hpo.tex}
            \caption{Fonctionnement général de l'optimisation des hyperparamètres}
       \end{figure}  
       \end{column}
            
   \end{columns}

   

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Formulation PBM %%%%%%%%%%%%%%%%%%
\begin{frame}{Formulation du problème}
    \begin{block}{Equation}
        \begin{equation}
            \eta^* \in \arg \max_{\eta \in \mathcal A}f(\eta), \quad f:\mathbb{R}^d \rightarrow \mathbb{R}
        \end{equation}
        Avec $\eta$ une solution de dimension $d$ et $f$ la fonction représentant l'entrainement et l'évaluation d'un modèle.
    \end{block}

    \begin{block}{Charactéristiques de la fonction $f$}
        \begin{itemize}
            \item Boite-noire : non dérivable
            \item Couteux : une évaluation se compte en dizaine de minutes
            \item Bruité : évaluer 2 fois la même solution peut donner un résultat différent
            \item Variables mixes : les variables sont de plusieurs type (entier, continu...)
        \end{itemize}
        
    \end{block}
    
   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Related works %%%%%%%%%%%%%%%%%%
\begin{frame}{Travaux connexes}
    \begin{figure}
        \centering
        \input{assets/tikz_picture/related_work.tex}
        \caption{Classification des travaux similaires}
   \end{figure} 
   
\end{frame}