\begin{frame}{Annexes \ref{ap:llm_architecture} : Transformer architecture}
    \label{ap:llm_architecture}
    \begin{figure}
        \centering
        \input{assets/annexes/transformer.tex}
        \caption{Illustration du mécanisme d'auto-attention : A droite le mécanisme complet, a gauche le \textit{Scaled Dot-product Attention}}
    \end{figure}
    
\end{frame}

\begin{frame}{Annexes \ref{ap:mha} : Multi-Head Attention}
    \label{ap:mha}
    \begin{figure}
        \centering
        \input{assets/annexes/mha.tex}
        \caption{Illustration du mécanisme d'auto-attention : A droite le mécanisme complet, a gauche le \textit{Scaled Dot-product Attention}}
    \end{figure}
    
\end{frame}

\begin{frame}{Annexes \ref{ap:lora} : Low Rank Adaptation (LoRA)}
    \label{ap:lora}
    \begin{figure}
        \centering
        \input{assets/annexes/lora.tex}
        \caption{Illustration de l'application du Low Rank Adaptation (LoRA)}
    \end{figure}
    
\end{frame}