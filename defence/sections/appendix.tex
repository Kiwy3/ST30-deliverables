%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Transformer %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:llm_architecture} : Transformer architecture}
    \label{ap:llm_architecture}
    \begin{figure}
        \centering
        \input{assets/annexes/transformer.tex}
        \caption{Illustration du mécanisme d'auto-attention : A droite le mécanisme complet, a gauche le \textit{Scaled Dot-product Attention}}
    \end{figure}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Multi-Head Attention %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:mha} : Multi-Head Attention}
    \label{ap:mha}
    \begin{figure}
        \centering
        \input{assets/annexes/mha.tex}
        \caption{Illustration du mécanisme d'auto-attention : A droite le mécanisme complet, a gauche le \textit{Scaled Dot-product Attention}}
    \end{figure}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Low Rank Adaptation %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:lora} : Low Rank Adaptation (LoRA)}
    \label{ap:lora}
    \begin{figure}
        \centering
        \input{assets/annexes/lora.tex}
        \caption{Illustration de l'application du Low Rank Adaptation (LoRA)}
    \end{figure}
    
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BO results %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:bo_results} : Résultats pour BO}
    \label{ap:bo_results}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SOO results %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:soo_results} : Résultats pour SOO}
    \label{ap:soo_results}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BaMSOO results %%%%%%%%%%%%%%%%%%
\begin{frame}{Annexe \ref{ap:bamsoo_results} : Résultats pour BaMSOO}
    \label{ap:bamsoo_results}
    
\end{frame}